
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/opt/homebrew/lib/python3.9/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /Users/distiller/project/pytorch/aten/src/ATen/native/TensorCompare.cpp:333.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
tensor([[ 0.0244,  0.5056,  0.3339,  0.3657,  0.5534, -0.1369,  0.4097,  1.2257,
          0.5953,  0.0194,  0.4674,  0.7272,  0.0656, -0.0112, -0.1469,  0.2255,
          0.0640,  0.3338,  0.4067,  0.3177,  0.3421,  0.1347, -0.1362,  0.2659,
         -0.1444,  0.1888,  0.7002,  0.4932,  0.3551,  0.1323,  0.2676, -0.3392],
        [ 0.0360,  0.5589,  0.2848,  0.1555, -0.0849, -0.1829,  0.1825,  0.4947,
          0.3317,  0.3079,  0.1727,  0.1498,  0.3321,  0.0138,  0.0773,  0.1772,
          0.1403,  0.2301,  0.2399,  0.1035,  0.2379,  0.2218,  0.1429, -0.0161,
          0.2981, -0.0658,  0.0460,  0.2831,  0.6459,  0.3468,  0.1283,  0.0525],
        [ 0.6587,  0.8231,  0.2629,  0.7210,  0.3434,  0.5793,  0.5656,  1.0771,
          0.7155,  0.7168,  0.6313,  0.7308,  0.5047,  0.7509,  0.7288,  0.4211,
          0.5113,  0.1892,  0.5344,  0.4392,  0.5472,  0.2920,  0.2422,  0.3513,
          0.3116,  0.7073,  0.5884,  0.2423,  0.5195,  0.7443,  0.3481,  0.4137],
        [-0.2387, -0.0093, -0.0358, -0.1261,  0.6045, -0.3203,  0.2280,  0.6434,
          0.3969, -0.1710,  0.0571,  0.6502, -0.1261, -0.1212, -0.4303,  0.1642,
         -0.0558,  0.3178,  0.2635,  0.0386,  0.0034, -0.0263, -0.2466,  0.1076,
         -0.3520, -0.0868,  0.4753,  0.4960,  0.2019,  0.0681,  0.2993, -0.1970],
        [ 0.0551,  0.0613, -0.0676, -0.3189, -0.2987, -0.0841,  0.0181, -0.3524,
         -0.1315,  0.1348, -0.5025,  0.0901,  0.1231, -0.0789, -0.2234,  0.1332,
          0.0378, -0.0681, -0.1682, -0.4406, -0.0836,  0.0780,  0.0498,  0.1077,
          0.0731, -0.3242, -0.1617,  0.0211,  0.2106,  0.2999, -0.1048, -0.0702],
        [ 0.4341,  0.2311, -0.1839,  0.1521,  0.2789,  0.4344,  0.3067,  0.3792,
          0.4014,  0.4493,  0.1053,  0.5767,  0.3901,  0.5637,  0.5226,  0.2826,
          0.3144,  0.1347,  0.2755,  0.0444,  0.1314,  0.0539,  0.0546,  0.2315,
          0.1811,  0.3545,  0.2864,  0.1294,  0.3279,  0.6030,  0.2642,  0.4402]],
       grad_fn=<StackBackward0>)
tensor([[ 7, 11],
        [28,  1],
        [ 7,  1],
        [11,  7],
        [29, 28],
        [29, 11]])