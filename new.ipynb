{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pipeline import Pipeline\n",
    "from Language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datasets import load_dataset\n",
    "from fugashi import Tagger\n",
    "# import nagisa\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer                           \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import libvoikko\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Reusing dataset parquet (C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('copenlu/answerable_tydiqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_english(text):\n",
    "    lower = [x.lower() for x in text]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    words = [stemmer.stem(word) for word in lower if not word in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_finnish(text):\n",
    "    lower = [x.lower() for x in text]\n",
    "    stop_words = set(stopwords.words('finnish'))\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "    words = [stemmer.stem(word) for word in lower if not word in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_japanese(text):\n",
    "    return [x.lower() for x in text]\n",
    "\n",
    "japanese_tagger = Tagger('-Owakati') # Tagger has initial startup overhead, therefore it is defined here and not in lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-42d8b8b0bba1f895.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-302ea1ccbac68b05.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-f1d195ad391e1424.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-69553e7e5be5ae7b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-5365d0f02bf56e62.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-10508a4f698d08e4.arrow\n"
     ]
    }
   ],
   "source": [
    "get_data = lambda language: dataset.filter(lambda x: x['language'] == language)\n",
    "\n",
    "# define languages\n",
    "languages = {\n",
    "    'english': Language(\n",
    "        name = 'english',\n",
    "        tokenizer = word_tokenize,\n",
    "        cleaner = clean_english,\n",
    "        pipeline = Pipeline(get_data('english'))\n",
    "        ),\n",
    "    'japanese': Language(\n",
    "        name = 'japanese',\n",
    "        tokenizer = lambda text : japanese_tagger.parse(text).split(\" \"), #lambda text: text.split(), #[word.surface for word in fugashi.Tagger()(text)],\n",
    "        cleaner = clean_japanese,\n",
    "        pipeline = Pipeline(get_data('japanese'))\n",
    "        ),\n",
    "    'finnish': Language(\n",
    "        name = 'finnish',\n",
    "        tokenizer = word_tokenize,\n",
    "        cleaner = clean_finnish,\n",
    "        pipeline = Pipeline(get_data('finnish'))\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-67bb50aaddc289fa.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-b7b4da070683f0ab.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-3457af876fbd303b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-c0bc4c45f0d3f1ea.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-102257cc42cad874.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-9e5537ea9b51c0d0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-c0bcdcaed636198a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-1812bbc8c80725a7.arrow\n",
      "100%|██████████| 8778/8778 [00:05<00:00, 1629.79ex/s]\n",
      "100%|██████████| 1036/1036 [00:00<00:00, 1840.04ex/s]\n",
      "100%|██████████| 8778/8778 [00:04<00:00, 2184.76ex/s]\n",
      "100%|██████████| 1036/1036 [00:00<00:00, 1720.94ex/s]\n",
      "100%|██████████| 8778/8778 [00:06<00:00, 1407.21ex/s]\n",
      "100%|██████████| 1036/1036 [00:00<00:00, 1681.69ex/s]\n",
      "100%|██████████| 8778/8778 [00:05<00:00, 1463.05ex/s]\n",
      "100%|██████████| 1036/1036 [00:00<00:00, 1403.80ex/s]\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-c8a0c5e1bbf50934.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-694fbba42355c700.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-51276bc181948adc.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-08746dffc3d61768.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-c62f2c80dd308771.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-4e0b6423b1327532.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-eaea1166327d4dd1.arrow\n",
      "Loading cached processed dataset at C:\\Users\\chris\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-6afd1a8255525df3.arrow\n"
     ]
    }
   ],
   "source": [
    "for language in languages.values():\n",
    "    language.pipeline.tokenize(language.tokenizer)\n",
    "    language.pipeline.clean(language.cleaner)\n",
    "    language.pipeline.label_answerable()\n",
    "    language.pipeline.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Language: english\n",
      "    Most frequent first words:\n",
      "    [('When', 2242), ('What', 2101), ('How', 1296), ('Who', 1058), ('Where', 486)]\n",
      "    Most frequent last words:\n",
      "    [('?', 7379), ('zombie', 2), ('metabolite', 2), ('\\\\', 2), ('BCE', 2)]\n",
      "    \n",
      "\n",
      "    Language: japanese\n",
      "    Most frequent first words:\n",
      "    [('日本', 392), ('『', 306), ('アメリカ', 106), ('世界', 94), ('第', 56)]\n",
      "    Most frequent last words:\n",
      "    [('？', 5920), ('いつ', 730), ('た', 608), ('どこ', 584), ('何', 448)]\n",
      "    \n",
      "\n",
      "    Language: finnish\n",
      "    Most frequent first words:\n",
      "    [('Milloin', 3519), ('Mikä', 2328), ('Missä', 1646), ('Kuka', 1619), ('Mitä', 1088)]\n",
      "    Most frequent last words:\n",
      "    [('?', 13689), ('tulitaistelussa', 2), ('tohtoriksi+', 2), ('syntynyt', 2), ('pinta-ala', 2)]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Find the most common first and last words in each language\n",
    "for language in languages.values():\n",
    "    count_words = lambda text: np.unique(text, return_counts=True) # Count occurences of words in text\n",
    "    sort_words = lambda word_count: np.argsort(word_count[1])[::-1] # Get list of sorted indices based on most frequent words\n",
    "    zip_words = lambda word_counts, sort_indices: list(zip(word_counts[0][sort_indices],word_counts[1][sort_indices])) # Zip the most frequent words with its number of occurences\n",
    "    def find_most_common(text):\n",
    "        \"\"\"Finds the most frequent words in a text together with its number of occurences\"\"\"\n",
    "        word_count = count_words(text)\n",
    "        return zip_words(word_count, sort_words(word_count))\n",
    "\n",
    "\n",
    "    tokenized_questions = language.pipeline.train_data['tokenized_question']\n",
    "    first = [sublist[0] for sublist in tokenized_questions]\n",
    "    last = [sublist[-1] for sublist in tokenized_questions]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Language: {language.name}\n",
    "    Most frequent first words:\n",
    "    {find_most_common(first)[:5]}\n",
    "    Most frequent last words:\n",
    "    {find_most_common(last)[:5]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6571931249154148\n",
      "Validation accuracy: 0.6373737373737374\n",
      "Training accuracy: 0.658464342674869\n",
      "Validation accuracy: 0.6486486486486487\n",
      "Training accuracy: 0.7238887672432669\n",
      "Validation accuracy: 0.727164887307236\n"
     ]
    }
   ],
   "source": [
    "for language in languages.values():\n",
    "    language.pipeline.train(LogisticRegression())\n",
    "    language.pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['麩', '菓子', 'は', '、', '麩', 'を', '主材', '料', 'と', 'し', 'た', '日本', 'の', '菓子', '。']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = Tagger('-Owakati')\n",
    "text = \"麩菓子は、麩を主材料とした日本の菓子。\"\n",
    "tagger.parse(text).split(\" \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e3bfd5707cea17bcbe380e30b983be31a9c4e73ed42697506103186bbe325e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
