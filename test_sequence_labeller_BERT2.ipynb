{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import torch\n",
    "import random\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchcrf import CRF\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
    "from typing import List, Tuple, AnyStr\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "import heapq\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocess import Preprocess\n",
    "from Pipeline import Pipeline\n",
    "from languages.English import English\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "english = English()\n",
    "\n",
    "preproccesor = Preprocess(english.tokenize, english.clean)\n",
    "data = Pipeline().get_data(language=english.name, preproccesor=preproccesor)\n",
    "train_data, validation_data = Pipeline().split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>is_answerable</th>\n",
       "      <th>tokenized_question</th>\n",
       "      <th>tokenized_plaintext</th>\n",
       "      <th>tokenized_answer</th>\n",
       "      <th>answer_end</th>\n",
       "      <th>cleaned_question</th>\n",
       "      <th>cleaned_plaintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>How many people die from AIDs each year?</td>\n",
       "      <td>AIDS</td>\n",
       "      <td>english</td>\n",
       "      <td>Mother-to-child transmission is another contri...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Epidemiology%20o...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[How, many, people, die, from, AIDs, each, yea...</td>\n",
       "      <td>[Mother-to-child, transmission, is, another, c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[mani, peopl, die, aid, year, ?]</td>\n",
       "      <td>[mother-to-child, transmiss, anoth, contribut,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>What language is spoken in Nepal?</td>\n",
       "      <td>Languages of Nepal</td>\n",
       "      <td>english</td>\n",
       "      <td>The official language of Nepal is Nepali, form...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Languages%20of%2...</td>\n",
       "      <td>6</td>\n",
       "      <td>Nepali</td>\n",
       "      <td>True</td>\n",
       "      <td>[What, language, is, spoken, in, Nepal, ?]</td>\n",
       "      <td>[The, official, language, of, Nepal, is, Nepal...</td>\n",
       "      <td>[Nepali]</td>\n",
       "      <td>6</td>\n",
       "      <td>[languag, spoken, nepal, ?]</td>\n",
       "      <td>[offici, languag, nepal, nepali, ,, former, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>Who is the Mayor of San Francisco?</td>\n",
       "      <td>Mayor of San Francisco</td>\n",
       "      <td>english</td>\n",
       "      <td>The current mayor is former District 5 Supervi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mayor%20of%20San...</td>\n",
       "      <td>15</td>\n",
       "      <td>London Breed</td>\n",
       "      <td>True</td>\n",
       "      <td>[Who, is, the, Mayor, of, San, Francisco, ?]</td>\n",
       "      <td>[The, current, mayor, is, former, District, 5,...</td>\n",
       "      <td>[London, Breed]</td>\n",
       "      <td>16</td>\n",
       "      <td>[mayor, san, francisco, ?]</td>\n",
       "      <td>[current, mayor, former, district, 5, supervis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7283</th>\n",
       "      <td>What percent of Denmark is Catholic?</td>\n",
       "      <td>Religion in Denmark</td>\n",
       "      <td>english</td>\n",
       "      <td>According to official statistics from January ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Religion%20in%20...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, percent, of, Denmark, is, Catholic, ?]</td>\n",
       "      <td>[According, to, official, statistics, from, Ja...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[percent, denmark, cathol, ?]</td>\n",
       "      <td>[accord, offici, statist, januari, 2018, ,, 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>What country does the island of Lesbos belong to?</td>\n",
       "      <td>Lesbos</td>\n",
       "      <td>english</td>\n",
       "      <td>In English and most other European languages, ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lesbos</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, country, does, the, island, of, Lesbos,...</td>\n",
       "      <td>[In, English, and, most, other, European, lang...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[countri, island, lesbo, belong, ?]</td>\n",
       "      <td>[english, european, languag, ,, includ, greek,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>Who is head coach of the Rouen Baseball 76?</td>\n",
       "      <td>Huskies de Rouen</td>\n",
       "      <td>english</td>\n",
       "      <td>European Champions Cup: 2-3 (4th Place)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Huskies%20de%20R...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[Who, is, head, coach, of, the, Rouen, Basebal...</td>\n",
       "      <td>[European, Champions, Cup, :, 2-3, (, 4th, Pla...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[head, coach, rouen, basebal, 76, ?]</td>\n",
       "      <td>[european, champion, cup, :, 2-3, (, 4th, plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>When was the first safety pin developed?</td>\n",
       "      <td>Safety pin</td>\n",
       "      <td>english</td>\n",
       "      <td>The laryngologist Dr. Chevalier Jackson devise...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Safety%20pin</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[When, was, the, first, safety, pin, developed...</td>\n",
       "      <td>[The, laryngologist, Dr., Chevalier, Jackson, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[first, safeti, pin, develop, ?]</td>\n",
       "      <td>[laryngologist, dr., chevali, jackson, devis, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>When was West Bathurst founded in Australia?</td>\n",
       "      <td>Bathurst, New South Wales</td>\n",
       "      <td>english</td>\n",
       "      <td>Bathurst is often referred to as the Gold Coun...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bathurst%2C%20Ne...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[When, was, West, Bathurst, founded, in, Austr...</td>\n",
       "      <td>[Bathurst, is, often, referred, to, as, the, G...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[west, bathurst, found, australia, ?]</td>\n",
       "      <td>[bathurst, often, refer, gold, countri, site, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>When was the character Sherlock Holmes first i...</td>\n",
       "      <td>Sherlock Holmes</td>\n",
       "      <td>english</td>\n",
       "      <td>First appearing in print in 1887's A Study in ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sherlock%20Holmes</td>\n",
       "      <td>5</td>\n",
       "      <td>1887</td>\n",
       "      <td>True</td>\n",
       "      <td>[When, was, the, character, Sherlock, Holmes, ...</td>\n",
       "      <td>[First, appearing, in, print, in, 1887, 's, A,...</td>\n",
       "      <td>[1887]</td>\n",
       "      <td>5</td>\n",
       "      <td>[charact, sherlock, holm, first, introduc, ?]</td>\n",
       "      <td>[first, appear, print, 1887, 's, studi, scarle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>When was the World Wide Web invented?</td>\n",
       "      <td>World Wide Web</td>\n",
       "      <td>english</td>\n",
       "      <td>The World Wide Web, also known as the WWW and ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/World%20Wide%20Web</td>\n",
       "      <td>58</td>\n",
       "      <td>1989</td>\n",
       "      <td>True</td>\n",
       "      <td>[When, was, the, World, Wide, Web, invented, ?]</td>\n",
       "      <td>[The, World, Wide, Web, ,, also, known, as, th...</td>\n",
       "      <td>[1989]</td>\n",
       "      <td>58</td>\n",
       "      <td>[world, wide, web, invent, ?]</td>\n",
       "      <td>[world, wide, web, ,, also, known, www, web, ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6700 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question_text  \\\n",
       "5415           How many people die from AIDs each year?   \n",
       "751                   What language is spoken in Nepal?   \n",
       "3774                 Who is the Mayor of San Francisco?   \n",
       "7283               What percent of Denmark is Catholic?   \n",
       "4382  What country does the island of Lesbos belong to?   \n",
       "...                                                 ...   \n",
       "4373        Who is head coach of the Rouen Baseball 76?   \n",
       "7891           When was the first safety pin developed?   \n",
       "4859       When was West Bathurst founded in Australia?   \n",
       "3264  When was the character Sherlock Holmes first i...   \n",
       "2732              When was the World Wide Web invented?   \n",
       "\n",
       "                 document_title language  \\\n",
       "5415                       AIDS  english   \n",
       "751          Languages of Nepal  english   \n",
       "3774     Mayor of San Francisco  english   \n",
       "7283        Religion in Denmark  english   \n",
       "4382                     Lesbos  english   \n",
       "...                         ...      ...   \n",
       "4373           Huskies de Rouen  english   \n",
       "7891                 Safety pin  english   \n",
       "4859  Bathurst, New South Wales  english   \n",
       "3264            Sherlock Holmes  english   \n",
       "2732             World Wide Web  english   \n",
       "\n",
       "                                     document_plaintext  \\\n",
       "5415  Mother-to-child transmission is another contri...   \n",
       "751   The official language of Nepal is Nepali, form...   \n",
       "3774  The current mayor is former District 5 Supervi...   \n",
       "7283  According to official statistics from January ...   \n",
       "4382  In English and most other European languages, ...   \n",
       "...                                                 ...   \n",
       "4373           European Champions Cup: 2-3 (4th Place)    \n",
       "7891  The laryngologist Dr. Chevalier Jackson devise...   \n",
       "4859  Bathurst is often referred to as the Gold Coun...   \n",
       "3264  First appearing in print in 1887's A Study in ...   \n",
       "2732  The World Wide Web, also known as the WWW and ...   \n",
       "\n",
       "                                           document_url  answer_start  \\\n",
       "5415  https://en.wikipedia.org/wiki/Epidemiology%20o...            -1   \n",
       "751   https://en.wikipedia.org/wiki/Languages%20of%2...             6   \n",
       "3774  https://en.wikipedia.org/wiki/Mayor%20of%20San...            15   \n",
       "7283  https://en.wikipedia.org/wiki/Religion%20in%20...            -1   \n",
       "4382               https://en.wikipedia.org/wiki/Lesbos            -1   \n",
       "...                                                 ...           ...   \n",
       "4373  https://en.wikipedia.org/wiki/Huskies%20de%20R...            -1   \n",
       "7891         https://en.wikipedia.org/wiki/Safety%20pin            -1   \n",
       "4859  https://en.wikipedia.org/wiki/Bathurst%2C%20Ne...            -1   \n",
       "3264    https://en.wikipedia.org/wiki/Sherlock%20Holmes             5   \n",
       "2732   https://en.wikipedia.org/wiki/World%20Wide%20Web            58   \n",
       "\n",
       "       answer_text  is_answerable  \\\n",
       "5415                        False   \n",
       "751         Nepali           True   \n",
       "3774  London Breed           True   \n",
       "7283                        False   \n",
       "4382                        False   \n",
       "...            ...            ...   \n",
       "4373                        False   \n",
       "7891                        False   \n",
       "4859                        False   \n",
       "3264          1887           True   \n",
       "2732          1989           True   \n",
       "\n",
       "                                     tokenized_question  \\\n",
       "5415  [How, many, people, die, from, AIDs, each, yea...   \n",
       "751          [What, language, is, spoken, in, Nepal, ?]   \n",
       "3774       [Who, is, the, Mayor, of, San, Francisco, ?]   \n",
       "7283      [What, percent, of, Denmark, is, Catholic, ?]   \n",
       "4382  [What, country, does, the, island, of, Lesbos,...   \n",
       "...                                                 ...   \n",
       "4373  [Who, is, head, coach, of, the, Rouen, Basebal...   \n",
       "7891  [When, was, the, first, safety, pin, developed...   \n",
       "4859  [When, was, West, Bathurst, founded, in, Austr...   \n",
       "3264  [When, was, the, character, Sherlock, Holmes, ...   \n",
       "2732    [When, was, the, World, Wide, Web, invented, ?]   \n",
       "\n",
       "                                    tokenized_plaintext tokenized_answer  \\\n",
       "5415  [Mother-to-child, transmission, is, another, c...               []   \n",
       "751   [The, official, language, of, Nepal, is, Nepal...         [Nepali]   \n",
       "3774  [The, current, mayor, is, former, District, 5,...  [London, Breed]   \n",
       "7283  [According, to, official, statistics, from, Ja...               []   \n",
       "4382  [In, English, and, most, other, European, lang...               []   \n",
       "...                                                 ...              ...   \n",
       "4373  [European, Champions, Cup, :, 2-3, (, 4th, Pla...               []   \n",
       "7891  [The, laryngologist, Dr., Chevalier, Jackson, ...               []   \n",
       "4859  [Bathurst, is, often, referred, to, as, the, G...               []   \n",
       "3264  [First, appearing, in, print, in, 1887, 's, A,...           [1887]   \n",
       "2732  [The, World, Wide, Web, ,, also, known, as, th...           [1989]   \n",
       "\n",
       "      answer_end                               cleaned_question  \\\n",
       "5415          -1               [mani, peopl, die, aid, year, ?]   \n",
       "751            6                    [languag, spoken, nepal, ?]   \n",
       "3774          16                     [mayor, san, francisco, ?]   \n",
       "7283          -1                  [percent, denmark, cathol, ?]   \n",
       "4382          -1            [countri, island, lesbo, belong, ?]   \n",
       "...          ...                                            ...   \n",
       "4373          -1           [head, coach, rouen, basebal, 76, ?]   \n",
       "7891          -1               [first, safeti, pin, develop, ?]   \n",
       "4859          -1          [west, bathurst, found, australia, ?]   \n",
       "3264           5  [charact, sherlock, holm, first, introduc, ?]   \n",
       "2732          58                  [world, wide, web, invent, ?]   \n",
       "\n",
       "                                      cleaned_plaintext  \n",
       "5415  [mother-to-child, transmiss, anoth, contribut,...  \n",
       "751   [offici, languag, nepal, nepali, ,, former, ca...  \n",
       "3774  [current, mayor, former, district, 5, supervis...  \n",
       "7283  [accord, offici, statist, januari, 2018, ,, 75...  \n",
       "4382  [english, european, languag, ,, includ, greek,...  \n",
       "...                                                 ...  \n",
       "4373  [european, champion, cup, :, 2-3, (, 4th, plac...  \n",
       "7891  [laryngologist, dr., chevali, jackson, devis, ...  \n",
       "4859  [bathurst, often, refer, gold, countri, site, ...  \n",
       "3264  [first, appear, print, 1887, 's, studi, scarle...  \n",
       "2732  [world, wide, web, ,, also, known, www, web, ,...  \n",
       "\n",
       "[6700 rows x 14 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_parquet('train_data.parquet')\n",
    "validation_data.to_parquet('validation_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('train_data.parquet')\n",
    "validation_data = pd.read_parquet('validation_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>is_answerable</th>\n",
       "      <th>tokenized_question</th>\n",
       "      <th>tokenized_plaintext</th>\n",
       "      <th>tokenized_answer</th>\n",
       "      <th>answer_end</th>\n",
       "      <th>cleaned_question</th>\n",
       "      <th>cleaned_plaintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>How many people die from AIDs each year?</td>\n",
       "      <td>AIDS</td>\n",
       "      <td>english</td>\n",
       "      <td>Mother-to-child transmission is another contri...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Epidemiology%20o...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[How, many, people, die, from, AIDs, each, yea...</td>\n",
       "      <td>[Mother-to-child, transmission, is, another, c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[mani, peopl, die, aid, year, ?]</td>\n",
       "      <td>[mother-to-child, transmiss, anoth, contribut,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>What language is spoken in Nepal?</td>\n",
       "      <td>Languages of Nepal</td>\n",
       "      <td>english</td>\n",
       "      <td>The official language of Nepal is Nepali, form...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Languages%20of%2...</td>\n",
       "      <td>6</td>\n",
       "      <td>Nepali</td>\n",
       "      <td>True</td>\n",
       "      <td>[What, language, is, spoken, in, Nepal, ?]</td>\n",
       "      <td>[The, official, language, of, Nepal, is, Nepal...</td>\n",
       "      <td>[Nepali]</td>\n",
       "      <td>6</td>\n",
       "      <td>[languag, spoken, nepal, ?]</td>\n",
       "      <td>[offici, languag, nepal, nepali, ,, former, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>Who is the Mayor of San Francisco?</td>\n",
       "      <td>Mayor of San Francisco</td>\n",
       "      <td>english</td>\n",
       "      <td>The current mayor is former District 5 Supervi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mayor%20of%20San...</td>\n",
       "      <td>15</td>\n",
       "      <td>London Breed</td>\n",
       "      <td>True</td>\n",
       "      <td>[Who, is, the, Mayor, of, San, Francisco, ?]</td>\n",
       "      <td>[The, current, mayor, is, former, District, 5,...</td>\n",
       "      <td>[London, Breed]</td>\n",
       "      <td>16</td>\n",
       "      <td>[mayor, san, francisco, ?]</td>\n",
       "      <td>[current, mayor, former, district, 5, supervis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7283</th>\n",
       "      <td>What percent of Denmark is Catholic?</td>\n",
       "      <td>Religion in Denmark</td>\n",
       "      <td>english</td>\n",
       "      <td>According to official statistics from January ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Religion%20in%20...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, percent, of, Denmark, is, Catholic, ?]</td>\n",
       "      <td>[According, to, official, statistics, from, Ja...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[percent, denmark, cathol, ?]</td>\n",
       "      <td>[accord, offici, statist, januari, 2018, ,, 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>What country does the island of Lesbos belong to?</td>\n",
       "      <td>Lesbos</td>\n",
       "      <td>english</td>\n",
       "      <td>In English and most other European languages, ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lesbos</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, country, does, the, island, of, Lesbos,...</td>\n",
       "      <td>[In, English, and, most, other, European, lang...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[countri, island, lesbo, belong, ?]</td>\n",
       "      <td>[english, european, languag, ,, includ, greek,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>Who is head coach of the Rouen Baseball 76?</td>\n",
       "      <td>Huskies de Rouen</td>\n",
       "      <td>english</td>\n",
       "      <td>European Champions Cup: 2-3 (4th Place)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Huskies%20de%20R...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[Who, is, head, coach, of, the, Rouen, Basebal...</td>\n",
       "      <td>[European, Champions, Cup, :, 2-3, (, 4th, Pla...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[head, coach, rouen, basebal, 76, ?]</td>\n",
       "      <td>[european, champion, cup, :, 2-3, (, 4th, plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>When was the first safety pin developed?</td>\n",
       "      <td>Safety pin</td>\n",
       "      <td>english</td>\n",
       "      <td>The laryngologist Dr. Chevalier Jackson devise...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Safety%20pin</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[When, was, the, first, safety, pin, developed...</td>\n",
       "      <td>[The, laryngologist, Dr., Chevalier, Jackson, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[first, safeti, pin, develop, ?]</td>\n",
       "      <td>[laryngologist, dr., chevali, jackson, devis, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>When was West Bathurst founded in Australia?</td>\n",
       "      <td>Bathurst, New South Wales</td>\n",
       "      <td>english</td>\n",
       "      <td>Bathurst is often referred to as the Gold Coun...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bathurst%2C%20Ne...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[When, was, West, Bathurst, founded, in, Austr...</td>\n",
       "      <td>[Bathurst, is, often, referred, to, as, the, G...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[west, bathurst, found, australia, ?]</td>\n",
       "      <td>[bathurst, often, refer, gold, countri, site, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>When was the character Sherlock Holmes first i...</td>\n",
       "      <td>Sherlock Holmes</td>\n",
       "      <td>english</td>\n",
       "      <td>First appearing in print in 1887's A Study in ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sherlock%20Holmes</td>\n",
       "      <td>5</td>\n",
       "      <td>1887</td>\n",
       "      <td>True</td>\n",
       "      <td>[When, was, the, character, Sherlock, Holmes, ...</td>\n",
       "      <td>[First, appearing, in, print, in, 1887, 's, A,...</td>\n",
       "      <td>[1887]</td>\n",
       "      <td>5</td>\n",
       "      <td>[charact, sherlock, holm, first, introduc, ?]</td>\n",
       "      <td>[first, appear, print, 1887, 's, studi, scarle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>When was the World Wide Web invented?</td>\n",
       "      <td>World Wide Web</td>\n",
       "      <td>english</td>\n",
       "      <td>The World Wide Web, also known as the WWW and ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/World%20Wide%20Web</td>\n",
       "      <td>58</td>\n",
       "      <td>1989</td>\n",
       "      <td>True</td>\n",
       "      <td>[When, was, the, World, Wide, Web, invented, ?]</td>\n",
       "      <td>[The, World, Wide, Web, ,, also, known, as, th...</td>\n",
       "      <td>[1989]</td>\n",
       "      <td>58</td>\n",
       "      <td>[world, wide, web, invent, ?]</td>\n",
       "      <td>[world, wide, web, ,, also, known, www, web, ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6700 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question_text  \\\n",
       "5415           How many people die from AIDs each year?   \n",
       "751                   What language is spoken in Nepal?   \n",
       "3774                 Who is the Mayor of San Francisco?   \n",
       "7283               What percent of Denmark is Catholic?   \n",
       "4382  What country does the island of Lesbos belong to?   \n",
       "...                                                 ...   \n",
       "4373        Who is head coach of the Rouen Baseball 76?   \n",
       "7891           When was the first safety pin developed?   \n",
       "4859       When was West Bathurst founded in Australia?   \n",
       "3264  When was the character Sherlock Holmes first i...   \n",
       "2732              When was the World Wide Web invented?   \n",
       "\n",
       "                 document_title language  \\\n",
       "5415                       AIDS  english   \n",
       "751          Languages of Nepal  english   \n",
       "3774     Mayor of San Francisco  english   \n",
       "7283        Religion in Denmark  english   \n",
       "4382                     Lesbos  english   \n",
       "...                         ...      ...   \n",
       "4373           Huskies de Rouen  english   \n",
       "7891                 Safety pin  english   \n",
       "4859  Bathurst, New South Wales  english   \n",
       "3264            Sherlock Holmes  english   \n",
       "2732             World Wide Web  english   \n",
       "\n",
       "                                     document_plaintext  \\\n",
       "5415  Mother-to-child transmission is another contri...   \n",
       "751   The official language of Nepal is Nepali, form...   \n",
       "3774  The current mayor is former District 5 Supervi...   \n",
       "7283  According to official statistics from January ...   \n",
       "4382  In English and most other European languages, ...   \n",
       "...                                                 ...   \n",
       "4373           European Champions Cup: 2-3 (4th Place)    \n",
       "7891  The laryngologist Dr. Chevalier Jackson devise...   \n",
       "4859  Bathurst is often referred to as the Gold Coun...   \n",
       "3264  First appearing in print in 1887's A Study in ...   \n",
       "2732  The World Wide Web, also known as the WWW and ...   \n",
       "\n",
       "                                           document_url  answer_start  \\\n",
       "5415  https://en.wikipedia.org/wiki/Epidemiology%20o...            -1   \n",
       "751   https://en.wikipedia.org/wiki/Languages%20of%2...             6   \n",
       "3774  https://en.wikipedia.org/wiki/Mayor%20of%20San...            15   \n",
       "7283  https://en.wikipedia.org/wiki/Religion%20in%20...            -1   \n",
       "4382               https://en.wikipedia.org/wiki/Lesbos            -1   \n",
       "...                                                 ...           ...   \n",
       "4373  https://en.wikipedia.org/wiki/Huskies%20de%20R...            -1   \n",
       "7891         https://en.wikipedia.org/wiki/Safety%20pin            -1   \n",
       "4859  https://en.wikipedia.org/wiki/Bathurst%2C%20Ne...            -1   \n",
       "3264    https://en.wikipedia.org/wiki/Sherlock%20Holmes             5   \n",
       "2732   https://en.wikipedia.org/wiki/World%20Wide%20Web            58   \n",
       "\n",
       "       answer_text  is_answerable  \\\n",
       "5415                        False   \n",
       "751         Nepali           True   \n",
       "3774  London Breed           True   \n",
       "7283                        False   \n",
       "4382                        False   \n",
       "...            ...            ...   \n",
       "4373                        False   \n",
       "7891                        False   \n",
       "4859                        False   \n",
       "3264          1887           True   \n",
       "2732          1989           True   \n",
       "\n",
       "                                     tokenized_question  \\\n",
       "5415  [How, many, people, die, from, AIDs, each, yea...   \n",
       "751          [What, language, is, spoken, in, Nepal, ?]   \n",
       "3774       [Who, is, the, Mayor, of, San, Francisco, ?]   \n",
       "7283      [What, percent, of, Denmark, is, Catholic, ?]   \n",
       "4382  [What, country, does, the, island, of, Lesbos,...   \n",
       "...                                                 ...   \n",
       "4373  [Who, is, head, coach, of, the, Rouen, Basebal...   \n",
       "7891  [When, was, the, first, safety, pin, developed...   \n",
       "4859  [When, was, West, Bathurst, founded, in, Austr...   \n",
       "3264  [When, was, the, character, Sherlock, Holmes, ...   \n",
       "2732    [When, was, the, World, Wide, Web, invented, ?]   \n",
       "\n",
       "                                    tokenized_plaintext tokenized_answer  \\\n",
       "5415  [Mother-to-child, transmission, is, another, c...               []   \n",
       "751   [The, official, language, of, Nepal, is, Nepal...         [Nepali]   \n",
       "3774  [The, current, mayor, is, former, District, 5,...  [London, Breed]   \n",
       "7283  [According, to, official, statistics, from, Ja...               []   \n",
       "4382  [In, English, and, most, other, European, lang...               []   \n",
       "...                                                 ...              ...   \n",
       "4373  [European, Champions, Cup, :, 2-3, (, 4th, Pla...               []   \n",
       "7891  [The, laryngologist, Dr., Chevalier, Jackson, ...               []   \n",
       "4859  [Bathurst, is, often, referred, to, as, the, G...               []   \n",
       "3264  [First, appearing, in, print, in, 1887, 's, A,...           [1887]   \n",
       "2732  [The, World, Wide, Web, ,, also, known, as, th...           [1989]   \n",
       "\n",
       "      answer_end                               cleaned_question  \\\n",
       "5415          -1               [mani, peopl, die, aid, year, ?]   \n",
       "751            6                    [languag, spoken, nepal, ?]   \n",
       "3774          16                     [mayor, san, francisco, ?]   \n",
       "7283          -1                  [percent, denmark, cathol, ?]   \n",
       "4382          -1            [countri, island, lesbo, belong, ?]   \n",
       "...          ...                                            ...   \n",
       "4373          -1           [head, coach, rouen, basebal, 76, ?]   \n",
       "7891          -1               [first, safeti, pin, develop, ?]   \n",
       "4859          -1          [west, bathurst, found, australia, ?]   \n",
       "3264           5  [charact, sherlock, holm, first, introduc, ?]   \n",
       "2732          58                  [world, wide, web, invent, ?]   \n",
       "\n",
       "                                      cleaned_plaintext  \n",
       "5415  [mother-to-child, transmiss, anoth, contribut,...  \n",
       "751   [offici, languag, nepal, nepali, ,, former, ca...  \n",
       "3774  [current, mayor, former, district, 5, supervis...  \n",
       "7283  [accord, offici, statist, januari, 2018, ,, 75...  \n",
       "4382  [english, european, languag, ,, includ, greek,...  \n",
       "...                                                 ...  \n",
       "4373  [european, champion, cup, :, 2-3, (, 4th, plac...  \n",
       "7891  [laryngologist, dr., chevali, jackson, devis, ...  \n",
       "4859  [bathurst, often, refer, gold, countri, site, ...  \n",
       "3264  [first, appear, print, 1887, 's, studi, scarle...  \n",
       "2732  [world, wide, web, ,, also, known, www, web, ,...  \n",
       "\n",
       "[6700 rows x 14 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/christianjensen/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/christianjensen/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/christianjensen/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/christianjensen/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/christianjensen/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/christianjensen/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/christianjensen/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tag_token(token, index, answer_start, answer_end):\n",
    "    \"\"\"Tag a token with the IOB format\"\"\"\n",
    "    if index == answer_start:\n",
    "        return 0 # B\n",
    "    elif answer_start < index <= answer_end:\n",
    "        return 1 # I\n",
    "    else:\n",
    "        return 2 # O\n",
    "\n",
    "def _tag_sentence(sentence, answer_start, answer_end):\n",
    "    \"\"\"Tag a sentence with the IOB format\"\"\"\n",
    "    return [\n",
    "        _tag_token(token, index, answer_start, answer_end)\n",
    "        for index, token in enumerate(sentence)\n",
    "    ]\n",
    "\n",
    "def _convert_to_iob(dataset):\n",
    "    \"\"\"Tag the dataset with the IOB format\"\"\"\n",
    "    dataset['tags'] = dataset.apply(lambda row: _tag_sentence(row['tokenized_plaintext'], row['answer_start'], row['answer_end']), axis=1)\n",
    "    return dataset\n",
    "\n",
    "def _tokenize_and_align_labels(batch):\n",
    "    \"\"\"Realign the labels to the tokenized inputs. This is due to the tokenizer splitting the words into subwords\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "                        batch['tokenized_question'],\n",
    "                        batch['tokenized_plaintext'],\n",
    "                        max_length=512,\n",
    "                        padding='max_length',\n",
    "                        truncation='only_second',\n",
    "                        is_split_into_words=True,\n",
    "                        return_tensors='pt'\n",
    "                        )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(batch[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        sep_index = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif sep_index == None: # Set all the tokens of the question to -100\n",
    "                label_ids.append(-100)\n",
    "                if tokenized_inputs['input_ids'][i][word_idx] == tokenizer.sep_token_id: # Find the index of the first SEP token to know where the question ends\n",
    "                    sep_index = word_idx\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx-sep_index]) # The tags are offset by the number of tokens in the question\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def extract_X(dataset, language: str = \"\"):\n",
    "    \"\"\"Extract features from the dataset\"\"\"\n",
    "    dataset = _convert_to_iob(dataset)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(dataset[['tokenized_question', 'tokenized_plaintext', 'tags']])\n",
    "    train_dataset = train_dataset.map(_tokenize_and_align_labels, batched=True)\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def extract_y(dataset, language: str = \"\"):\n",
    "    \"\"\"Extract the labels from the dataset\"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42232e48835d4f66b5fe986d2e99a717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bc7ef7eced4f3186ca800b6406be28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = extract_X(train_data, 'english')\n",
    "X_val = extract_X(validation_data, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    \"test-bert\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be85771680e4528807c0ab16e2880f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokenized_question', 'tokenized_plaintext', 'tags', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6700\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0, None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(X_train['labels'][0], X_train['labels'][0], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    binarizer = MultiLabelBinarizer().fit(true_labels)\n",
    "\n",
    "    true_predictions = binarizer.transform(true_predictions)\n",
    "    true_labels = binarizer.transform(true_labels)\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(true_labels, true_predictions, average='macro', labels=[0,1,2])\n",
    "    return {\n",
    "        \"precision\": P,\n",
    "        \"recall\": R,\n",
    "        \"f1\": F1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=X_train,\n",
    "    eval_dataset=X_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokenized_plaintext, tags, tokenized_question. If __index_level_0__, tokenized_plaintext, tags, tokenized_question are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/opt/homebrew/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6700\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 630\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristian2903\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/christianjensen/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2 - Projects/NLP Course/NLP-2022/wandb/run-20221022_133208-1z2jixz6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/christian2903/huggingface/runs/1z2jixz6\" target=\"_blank\">test-bert</a></strong> to <a href=\"https://wandb.ai/christian2903/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477523c770044b8d862d279277acd5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/homebrew/lib/python3.9/site-packages/transformers/data/data_collator.py:318: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/opt/homebrew/lib/python3.9/site-packages/transformers/data/data_collator.py:329: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_65851/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m         )\n\u001b[0;32m-> 1521\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1522\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1761\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m                 if (\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2499\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1757\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1759\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1760\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1023\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    609\u001b[0m                 )\n\u001b[1;32m    610\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    612\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
