{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Model import Model\n",
    "from models.GPT2CBOWLogistic import GPT2CBOWLogistic\n",
    "from models.GPT2Generator import GPT2Generator\n",
    "from models.Logistic.BOWLogistic import BOWLogistic\n",
    "from models.MLP.BOWMLP import BOWMLP\n",
    "from models.MLP.CBOW_BOWMLP import CBOW_BOWMLP\n",
    "from models.MLP.CBOWMLP import CBOWMLP\n",
    "from models.RandomForest.BOWRandomForest import BOWRandomForest\n",
    "from models.RandomForest.CBOW_BOWRandomForest import CBOW_BOWRandomForest\n",
    "from models.RandomForest.CBOWRandomForest import CBOWRandomForest\n",
    "from models.Logistic.CBOW_BOWLogistic import CBOW_BOWLogistic\n",
    "from models.Logistic.CBOWLogistic import CBOWLogistic\n",
    "from models.XGBoost.BOWXGBoost import BOWXGBoost\n",
    "from models.XGBoost.CBOW_BOWXGBoost import CBOW_BOWXGBoost\n",
    "from models.XGBoost.CBOWXGBoost import CBOWXGBoost\n",
    "\n",
    "from models.SequenceLabeller_BiLSTM_CRF import SequenceLabeller_BiLSTM_CRF\n",
    "from models.SequenceLabeller_BiLSTM_CRF_Beam import SequenceLabeller_BiLSTM_CRF_Beam\n",
    "from models.SequenceLabeller_BERT import SequenceLabeller_BERT\n",
    "\n",
    "\n",
    "from languages.LanguageModel import LanguageModel\n",
    "from DataExploration import DataExploration\n",
    "# from languages.Japanese import Japanese\n",
    "from languages.English import English\n",
    "from languages.Finnish import Finnish\n",
    "from Preprocess import Preprocess\n",
    "from Pipeline import Pipeline\n",
    "from typing import List\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently \n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is used to minimize the clutter in the console\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "# Define the languages to be used\n",
    "languages: List[LanguageModel] = [\n",
    "    English(),\n",
    "    Finnish(),\n",
    "    # Japanese()\n",
    "]\n",
    "\n",
    "# gpt2Generator = GPT2Generator()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_headline(language: str):\n",
    "    print(f'\\n\\n--- Language: {language} ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "all_data = {}\n",
    "\n",
    "for language in languages:\n",
    "    pipeline = Pipeline()\n",
    "\n",
    "    # Get the preprocessed data and split it into training and validation data\n",
    "    preprocessor = Preprocess(language.tokenize, language.clean)\n",
    "    data = pipeline.get_data(language=language.name, preproccesor=preprocessor)\n",
    "    train_data, validation_data = pipeline.split_data(data)\n",
    "\n",
    "    all_data[language.name] = {\n",
    "        \"train_data\": train_data,\n",
    "        \"validation_data\": validation_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Language: english ---\n",
      "\n",
      "Most frequent first words:\n",
      "[('When', 1994), ('What', 1967), ('How', 1188), ('Who', 945), ('Where', 419)]\n",
      "Most frequent last words:\n",
      "[('?', 6693), ('\\\\', 2), ('BCE', 2), (\"''\", 2), ('metabolite', 1)]\n",
      "\n",
      "\n",
      "\n",
      "--- Language: finnish ---\n",
      "\n",
      "Most frequent first words:\n",
      "[('Milloin', 3210), ('Mikä', 2048), ('Missä', 1508), ('Kuka', 1435), ('Mitä', 928)]\n",
      "Most frequent last words:\n",
      "[('?', 12292), ('tohtoriksi+', 2), ('syntynyt', 2), ('pinta-ala', 2), ('=', 2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the data for each language\n",
    "for language in languages:\n",
    "    print_headline(language.name)\n",
    "    data_exploration = DataExploration(all_data[language.name][\"train_data\"])\n",
    "    data_exploration.find_frequent_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Question Classification\n",
    "Binary classfiers that only takes features based on the question, context document and combinations of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print_headline(language.name)\n",
    "\n",
    "    feature_based_classifiers = [\n",
    "        BOWRandomForest(language.name),\n",
    "        BOWMLP(language.name),\n",
    "        BOWLogistic(language.name),\n",
    "        BOWXGBoost(language.name)\n",
    "    ]\n",
    "    \n",
    "    for classifier in feature_based_classifiers:\n",
    "        print(f'--- Classifier: {classifier.__name__} ---')\n",
    "        train_data = all_data[language.name][\"train_data\"]\n",
    "        validation_data = all_data[language.name][\"validation_data\"]\n",
    "\n",
    "        pipeline = Pipeline()\n",
    "        pipeline.train(\n",
    "            model=classifier,\n",
    "            X=classifier.extract_X(train_data),\n",
    "            y=classifier.extract_y(train_data)\n",
    "        )\n",
    "\n",
    "        pipeline.evaluate(\n",
    "            model=classifier,\n",
    "            X=classifier.extract_X(validation_data),\n",
    "            y=classifier.extract_y(validation_data)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Learning\n",
    "Extension of our binary question classifers to also include features based on continous vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print_headline(language.name)\n",
    "\n",
    "    continous_based_classifiers = [\n",
    "        CBOW_BOWRandomForest(language.name),\n",
    "        CBOW_BOWMLP(language.name),\n",
    "        CBOW_BOWLogistic(language.name),\n",
    "        CBOW_BOWXGBoost(language.name)\n",
    "    ]\n",
    "    \n",
    "    for classifier in feature_based_classifiers:\n",
    "        print(f'--- Classifier: {classifier.__name__} ---')\n",
    "        train_data = all_data[language.name][\"train_data\"]\n",
    "        validation_data = all_data[language.name][\"validation_data\"]\n",
    "\n",
    "        pipeline = Pipeline()\n",
    "        pipeline.train(\n",
    "            model=classifier,\n",
    "            X=classifier.extract_X(train_data),\n",
    "            y=classifier.extract_y(train_data)\n",
    "        )\n",
    "\n",
    "        pipeline.evaluate(\n",
    "            model=classifier,\n",
    "            X=classifier.extract_X(validation_data),\n",
    "            y=classifier.extract_y(validation_data)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also test how the performance if only the continous representations was to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print_headline(language.name)\n",
    "\n",
    "    continous_based_classifiers = [\n",
    "        CBOWRandomForest(language.name),\n",
    "        CBOWMLP(language.name),\n",
    "        CBOWLogistic(language.name),\n",
    "        CBOWXGBoost(language.name)\n",
    "    ]\n",
    "    \n",
    "    for classifier in feature_based_classifiers:\n",
    "        print(f'--- Classifier: {classifier.__name__} ---')\n",
    "        train_data = all_data[language.name][\"train_data\"]\n",
    "        validation_data = all_data[language.name][\"validation_data\"]\n",
    "\n",
    "        pipeline = Pipeline()\n",
    "        pipeline.train(\n",
    "            model=classifier,\n",
    "            X=classifier.extract_X(train_data),\n",
    "            y=classifier.extract_y(train_data)\n",
    "        )\n",
    "\n",
    "        pipeline.evaluate(\n",
    "            model=classifier,\n",
    "            X=classifier.extract_X(validation_data),\n",
    "            y=classifier.extract_y(validation_data)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling\n",
    "Extension to the classifiers in which word/sentence representations are instead extracted from neural language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print_headline(language.name)\n",
    "\n",
    "    classifier = GPT2CBOWLogistic(language.name)\n",
    "\n",
    "    train_data = all_data[language.name][\"train_data\"]\n",
    "    validation_data = all_data[language.name][\"validation_data\"]\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.train(\n",
    "        model=classifier,\n",
    "        X=classifier.extract_X(train_data),\n",
    "        y=classifier.extract_y(train_data)\n",
    "    )\n",
    "\n",
    "    pipeline.evaluate(\n",
    "        model=classifier,\n",
    "        X=classifier.extract_X(validation_data),\n",
    "        y=classifier.extract_y(validation_data)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to sample from these language models to see what kinds of sentences they generate. Moreover we measure the performance on the TyDi QA validation data with a commonly used language model evaluations metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_beginning = {\n",
    "    'english': ['When', 'What', 'How'],\n",
    "    'finnish': ['Milloin', 'Mikä', 'Missä'],\n",
    "    'japanese': ['日本', '『', 'アメリカ']\n",
    "}\n",
    "\n",
    "for language in languages:\n",
    "    print_headline(language.name)\n",
    "\n",
    "    model = GPT2Generator(language.name)\n",
    "\n",
    "    train_data = all_data[language.name][\"train_data\"]\n",
    "    validation_data = all_data[language.name][\"validation_data\"]\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.train(\n",
    "        model=classifier,\n",
    "        X=classifier.extract_X(train_data),\n",
    "        y=classifier.extract_y(train_data)\n",
    "    )\n",
    "\n",
    "    for starting_word in question_beginning[language.name]:\n",
    "        model.generate_text(f'Question: {starting_word}')\n",
    "    model.get_perplexity(model.extract_X(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis and Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labelling\n",
    "We implement a sequence labeller, which predicts which parts of a paragraph are likel part of the answer to the corresponding question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print_headline(language.name)\n",
    "\n",
    "    sequence_labellers = [\n",
    "        SequenceLabeller_BiLSTM_CRF(language.name),\n",
    "        SequenceLabeller_BERT(language.name),\n",
    "    ]\n",
    "\n",
    "    for sequence_labeller in sequence_labellers:\n",
    "        print(f'--- Sequence Labeller: {sequence_labeller.__name__} ---')\n",
    "        train_data = all_data[language.name][\"train_data\"]\n",
    "        validation_data = all_data[language.name][\"validation_data\"]\n",
    "\n",
    "        pipeline = Pipeline()\n",
    "        pipeline.train(\n",
    "            model=sequence_labeller,\n",
    "            X=sequence_labeller.extract_X(train_data),\n",
    "            y=sequence_labeller.extract_y(train_data)\n",
    "        )\n",
    "\n",
    "        pipeline.evaluate(\n",
    "            model=sequence_labeller,\n",
    "            X=sequence_labeller.extract_X(validation_data),\n",
    "            y=sequence_labeller.extract_y(validation_data)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an extension to the sequence labeller which uses beam search to select the optimal sequence of labels for the location of the answer in the text. Analyse how the performance of this system differs with beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    print_headline(language.name)\n",
    "\n",
    "    sequence_labeller = SequenceLabeller_BiLSTM_CRF_Beam(language.name)\n",
    "\n",
    "    train_data = all_data[language.name][\"train_data\"]\n",
    "    validation_data = all_data[language.name][\"validation_data\"]\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.train(\n",
    "        model=sequence_labeller,\n",
    "        X=sequence_labeller.extract_X(train_data),\n",
    "        y=sequence_labeller.extract_y(train_data)\n",
    "    )\n",
    "\n",
    "    num_beams = [1, 2, 3]\n",
    "\n",
    "    for beam in num_beams:\n",
    "        sequence_labeller.beam_size = beam\n",
    "        pipeline.evaluate(\n",
    "            model=sequence_labeller,\n",
    "            X=sequence_labeller.extract_X(validation_data),\n",
    "            y=sequence_labeller.extract_y(validation_data)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative investigation of the predicted answer spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing our binary question system with a multilingual encoder instead of the monolingual ones. With this we perform zero-shot cross-lingual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutli-lingual binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement our sequence tagger with a multilingual encoding and perform zero-shot cross-lingual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Training on english ---\n",
      "--- Validating on finnish ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a3044600df43c08fef85b8c65c2832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokenized_question, tags, tokenized_plaintext, __index_level_0__. If tokenized_question, tags, tokenized_plaintext, __index_level_0__ are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/opt/homebrew/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6700\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1260\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristian2903\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/christianjensen/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2 - Projects/NLP Course/QuestionAnswering/wandb/run-20221024_102824-1huw5l1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/christian2903/huggingface/runs/1huw5l1y\" target=\"_blank\">sequence-labeller</a></strong> to <a href=\"https://wandb.ai/christian2903/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febf103eb6f84966a2b0fdfb9c5dc833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/homebrew/lib/python3.9/site-packages/transformers/data/data_collator.py:318: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/opt/homebrew/lib/python3.9/site-packages/transformers/data/data_collator.py:329: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_2767/1925440381.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     pipeline.train(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_labeller\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_labeller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2 - Projects/NLP Course/QuestionAnswering/Pipeline.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, X, y)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;34m\"\"\"Train the model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training the model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train score: {model.evaluate(X, y)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2 - Projects/NLP Course/QuestionAnswering/models/SequenceLabeller_BERT.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m         )\n\u001b[0;32m-> 1521\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1522\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1761\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m                 if (\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2515\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hook_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Zero shot classification\n",
    "for language in languages:\n",
    "    print(f'\\n\\n--- Training on {language.name} ---')\n",
    "\n",
    "    val_languages = [l.name for l in languages if l.name != language.name]\n",
    "    print(f'--- Validating on {\" and \".join(val_languages)} ---')\n",
    "\n",
    "    sequence_labeller = SequenceLabeller_BERT('multilingual', {})\n",
    "\n",
    "    train_data = all_data[language.name][\"train_data\"]\n",
    "    validation_data = all_data[language.name][\"validation_data\"]\n",
    "\n",
    "    val_df = [all_data[val_name]['validation_data'] for val_name in val_languages]\n",
    "\n",
    "    validation_data = pd.concat(val_df, ignore_index=True, axis=0)\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.train(\n",
    "        model=sequence_labeller,\n",
    "        X=sequence_labeller.extract_X(train_data),\n",
    "        y=sequence_labeller.extract_y(train_data)\n",
    "    )\n",
    "\n",
    "    pipeline.evaluate(\n",
    "        model=sequence_labeller,\n",
    "        X=sequence_labeller.extract_X(validation_data),\n",
    "        y=sequence_labeller.extract_y(validation_data)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
