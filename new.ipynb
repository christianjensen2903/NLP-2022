{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pipeline import Pipeline\n",
    "from Language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from add_transformation import add_transformation\n",
    "from datasets import load_dataset\n",
    "# import fugashi\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import libvoikko\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christianjensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-2c79d6e77df16c2a\n",
      "Reusing dataset parquet (/Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 175.75it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('copenlu/answerable_tydiqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-966bf95a10f76383.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e0b012db28a0a682.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-57253105caae3cc1.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-efce6354aa2f51ed.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b5a509467a2a492a.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ead531b3ac5c84fa.arrow\n"
     ]
    }
   ],
   "source": [
    "get_data = lambda language: dataset.filter(lambda x: x['language'] == language)\n",
    "\n",
    "# define languages\n",
    "languages = {\n",
    "    'english': Language(\n",
    "        name = 'english',\n",
    "        tokenizer = word_tokenize,\n",
    "        cleaner = lambda text: text.lower(),\n",
    "        pipeline = Pipeline(get_data('english'))\n",
    "        ),\n",
    "    'japanese': Language(\n",
    "        name = 'japanese',\n",
    "        tokenizer = lambda text: text.split(), #[word.surface for word in fugashi.Tagger()(text)],\n",
    "        cleaner = lambda text: text.lower(),\n",
    "        pipeline = Pipeline(get_data('japanese'))\n",
    "        ),\n",
    "    'finnish': Language(\n",
    "        name = 'finnish',\n",
    "        tokenizer = word_tokenize,\n",
    "        cleaner = lambda text: text.lower(),\n",
    "        pipeline = Pipeline(get_data('finnish'))\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7389/7389 [00:00<00:00, 11146.69ex/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 11723.47ex/s]\n",
      "100%|██████████| 7389/7389 [00:00<00:00, 19444.12ex/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 23437.02ex/s]\n",
      "100%|██████████| 8778/8778 [00:00<00:00, 15294.06ex/s]\n",
      "100%|██████████| 1036/1036 [00:00<00:00, 16234.64ex/s]\n",
      "100%|██████████| 8778/8778 [00:00<00:00, 23260.69ex/s]\n",
      "100%|██████████| 1036/1036 [00:00<00:00, 22116.86ex/s]\n",
      "100%|██████████| 13701/13701 [00:01<00:00, 11165.65ex/s]\n",
      "100%|██████████| 1686/1686 [00:00<00:00, 11489.47ex/s]\n",
      "100%|██████████| 13701/13701 [00:00<00:00, 22326.37ex/s]\n",
      "100%|██████████| 1686/1686 [00:00<00:00, 21959.71ex/s]\n"
     ]
    }
   ],
   "source": [
    "for language in languages.values():\n",
    "    language.pipeline.tokenize(language.tokenizer)\n",
    "    language.pipeline.label_answerable()\n",
    "    language.pipeline.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Language: english\n",
      "    Most frequent first word:\n",
      "    [('When', 2242), ('What', 2101), ('How', 1296), ('Who', 1058), ('Where', 486)]\n",
      "    Most frequent last word:\n",
      "    [('?', 7379), ('zombie', 2), ('metabolite', 2), ('\\\\', 2), ('BCE', 2)]\n",
      "    \n",
      "\n",
      "    Language: japanese\n",
      "    Most frequent first word:\n",
      "    [('朝比奈', 6), ('三原', 6), ('孫', 6), ('PlayStation', 6), ('加藤', 6)]\n",
      "    Most frequent last word:\n",
      "    [('の大きさは？', 4), ('の面積は？', 4), ('はいつ設立した？', 4), ('ＹＡＭＡＨＡがピアノの生産を始めたのはいつ', 2), ('ソ連が崩壊したのはいつ', 2)]\n",
      "    \n",
      "\n",
      "    Language: finnish\n",
      "    Most frequent first word:\n",
      "    [('Milloin', 3519), ('Mikä', 2328), ('Missä', 1646), ('Kuka', 1619), ('Mitä', 1088)]\n",
      "    Most frequent last word:\n",
      "    [('?', 13689), ('tulitaistelussa', 2), ('tohtoriksi+', 2), ('syntynyt', 2), ('pinta-ala', 2)]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Find the most common first and last words in each language\n",
    "for language in languages.values():\n",
    "    count_words = lambda text: np.unique(text, return_counts=True) # Count occurences of words in text\n",
    "    sort_words = lambda word_count: np.argsort(word_count[1])[::-1] # Get list of sorted indices based on most frequent words\n",
    "    zip_words = lambda word_counts, sort_indices: list(zip(word_counts[0][sort_indices],word_counts[1][sort_indices])) # Zip the most frequent words with its number of occurences\n",
    "    def find_most_common(text):\n",
    "        \"\"\"Finds the most frequent words in a text together with its number of occurences\"\"\"\n",
    "        word_count = count_words(text)\n",
    "        return zip_words(word_count, sort_words(word_count))\n",
    "\n",
    "\n",
    "    tokenized_questions = language.pipeline.train['tokenized_question']\n",
    "    first = [sublist[0] for sublist in tokenized_questions]\n",
    "    last = [sublist[-1] for sublist in tokenized_questions]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Language: {language.name}\n",
    "    Most frequent first words:\n",
    "    {find_most_common(first)[:5]}\n",
    "    Most frequent last words:\n",
    "    {find_most_common(last)[:5]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages.values():\n",
    "    language.pipeline.train(LogisticRegression())\n",
    "    language.pipeline.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
