{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pipeline import Pipeline\n",
    "from Language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datasets import load_dataset\n",
    "# import fugashi\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer                           \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import libvoikko\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christianjensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christianjensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Reusing dataset parquet (/Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 494.93it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('copenlu/answerable_tydiqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_english(text):\n",
    "    lower = [x.lower() for x in text]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    words = [stemmer.stem(word) for word in lower if not word in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_finnish(text):\n",
    "    lower = [x.lower() for x in text]\n",
    "    stop_words = set(stopwords.words('finnish'))\n",
    "    stemmer = SnowballStemmer(\"finnish\")\n",
    "    words = [stemmer.stem(word) for word in lower if not word in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_japanese(text):\n",
    "    return [x.lower() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-42d8b8b0bba1f895.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-302ea1ccbac68b05.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f1d195ad391e1424.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-69553e7e5be5ae7b.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5365d0f02bf56e62.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-10508a4f698d08e4.arrow\n"
     ]
    }
   ],
   "source": [
    "get_data = lambda language: dataset.filter(lambda x: x['language'] == language)\n",
    "\n",
    "# define languages\n",
    "languages = {\n",
    "    'english': Language(\n",
    "        name = 'english',\n",
    "        tokenizer = word_tokenize,\n",
    "        cleaner = clean_english,\n",
    "        pipeline = Pipeline(get_data('english'))\n",
    "        ),\n",
    "    'japanese': Language(\n",
    "        name = 'japanese',\n",
    "        tokenizer = lambda text: text.split(), #[word.surface for word in fugashi.Tagger()(text)],\n",
    "        cleaner = clean_japanese,\n",
    "        pipeline = Pipeline(get_data('japanese'))\n",
    "        ),\n",
    "    'finnish': Language(\n",
    "        name = 'finnish',\n",
    "        tokenizer = word_tokenize,\n",
    "        cleaner = clean_finnish,\n",
    "        pipeline = Pipeline(get_data('finnish'))\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-0f6f8d26547f1702.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5fff5b668285e1ac.arrow\n",
      "Parameter 'function'=<function Pipeline.clean.<locals>.clean_row at 0x15ba371f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 7389/7389 [00:04<00:00, 1492.10ex/s]\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5314b9aaa49aa7b7.arrow\n",
      "100%|██████████| 7389/7389 [00:01<00:00, 6187.08ex/s]\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e17c8e7e830865ad.arrow\n",
      "100%|██████████| 7389/7389 [00:01<00:00, 5990.08ex/s]\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-fa62df6fcc0bf399.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5288f639633b1fc4.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-41a59212354578e6.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e6a6be7670aaa6dd.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-0f5c09be99f20c6b.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6a2807f1831881fe.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-80d5dbc9b5446b64.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-1634b54f4eee7f22.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-2b5667aba05acf4b.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-351ee8bba8cbd394.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b777f0617718f06b.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9081d06fe810cce1.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-dac444a5ea9e2488.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-094dfe12209faf10.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-facef78b239ec469.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4dfe57f85ff92425.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-aba5b3a331def38f.arrow\n"
     ]
    }
   ],
   "source": [
    "for language in languages.values():\n",
    "    language.pipeline.tokenize(language.tokenizer)\n",
    "    language.pipeline.clean(language.cleaner)\n",
    "    language.pipeline.label_answerable()\n",
    "    language.pipeline.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Language: english\n",
      "    Most frequent first words:\n",
      "    [('When', 2242), ('What', 2101), ('How', 1296), ('Who', 1058), ('Where', 486)]\n",
      "    Most frequent last words:\n",
      "    [('?', 7379), ('zombie', 2), ('metabolite', 2), ('\\\\', 2), ('BCE', 2)]\n",
      "    \n",
      "\n",
      "    Language: japanese\n",
      "    Most frequent first words:\n",
      "    [('朝比奈', 6), ('三原', 6), ('孫', 6), ('PlayStation', 6), ('加藤', 6)]\n",
      "    Most frequent last words:\n",
      "    [('の大きさは？', 4), ('の面積は？', 4), ('はいつ設立した？', 4), ('ＹＡＭＡＨＡがピアノの生産を始めたのはいつ', 2), ('ソ連が崩壊したのはいつ', 2)]\n",
      "    \n",
      "\n",
      "    Language: finnish\n",
      "    Most frequent first words:\n",
      "    [('Milloin', 3519), ('Mikä', 2328), ('Missä', 1646), ('Kuka', 1619), ('Mitä', 1088)]\n",
      "    Most frequent last words:\n",
      "    [('?', 13689), ('tulitaistelussa', 2), ('tohtoriksi+', 2), ('syntynyt', 2), ('pinta-ala', 2)]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Find the most common first and last words in each language\n",
    "for language in languages.values():\n",
    "    count_words = lambda text: np.unique(text, return_counts=True) # Count occurences of words in text\n",
    "    sort_words = lambda word_count: np.argsort(word_count[1])[::-1] # Get list of sorted indices based on most frequent words\n",
    "    zip_words = lambda word_counts, sort_indices: list(zip(word_counts[0][sort_indices],word_counts[1][sort_indices])) # Zip the most frequent words with its number of occurences\n",
    "    def find_most_common(text):\n",
    "        \"\"\"Finds the most frequent words in a text together with its number of occurences\"\"\"\n",
    "        word_count = count_words(text)\n",
    "        return zip_words(word_count, sort_words(word_count))\n",
    "\n",
    "\n",
    "    tokenized_questions = language.pipeline.train_data['tokenized_question']\n",
    "    first = [sublist[0] for sublist in tokenized_questions]\n",
    "    last = [sublist[-1] for sublist in tokenized_questions]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Language: {language.name}\n",
    "    Most frequent first words:\n",
    "    {find_most_common(first)[:5]}\n",
    "    Most frequent last words:\n",
    "    {find_most_common(last)[:5]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6571931249154148\n",
      "Validation accuracy: 0.6373737373737374\n",
      "Training accuracy: 0.5061517429938482\n",
      "Validation accuracy: 0.502895752895753\n",
      "Training accuracy: 0.7238887672432669\n",
      "Validation accuracy: 0.727164887307236\n"
     ]
    }
   ],
   "source": [
    "for language in languages.values():\n",
    "    language.pipeline.train(LogisticRegression())\n",
    "    language.pipeline.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
