{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 11:02:44.367472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from models.Model import Model\n",
    "from models.Logistic.MultiGPT2Logistic import MultiGPT2Logistic\n",
    "from languages.LanguageModel import LanguageModel\n",
    "from DataExploration import DataExploration\n",
    "# from languages.Japanese import Japanese\n",
    "from languages.English import English\n",
    "from languages.Finnish import Finnish\n",
    "from Preprocess import Preprocess\n",
    "from Pipeline import Pipeline\n",
    "from typing import List\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently \n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is used to minimize the clutter in the console\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "# Define the languages to be used\n",
    "languages: List[LanguageModel] = [\n",
    "    English(),\n",
    "    Finnish(),\n",
    "    # Japanese()\n",
    "]\n",
    "\n",
    "# gpt2Generator = GPT2Generator()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "all_data = {}\n",
    "\n",
    "for language in languages:\n",
    "    pipeline = Pipeline()\n",
    "\n",
    "    # Get the preprocessed data and split it into training and validation data\n",
    "    preprocessor = Preprocess(language.tokenize, language.clean)\n",
    "    data = pipeline.get_data(language=language.name, preproccesor=preprocessor)\n",
    "    train_data, validation_data = pipeline.split_data(data)\n",
    "\n",
    "    all_data[language.name] = {\n",
    "        \"train_data\": train_data,\n",
    "        \"validation_data\": validation_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>is_answerable</th>\n",
       "      <th>tokenized_question</th>\n",
       "      <th>tokenized_plaintext</th>\n",
       "      <th>cleaned_question</th>\n",
       "      <th>cleaned_plaintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>When did the episode The Sue Sylvester Shuffle...</td>\n",
       "      <td>The Sue Sylvester Shuffle</td>\n",
       "      <td>english</td>\n",
       "      <td>Musical performances also attracted mixed comm...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The%20Sue%20Sylv...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[When, did, the, episode, The, Sue, Sylvester,...</td>\n",
       "      <td>[Musical, performances, also, attracted, mixed...</td>\n",
       "      <td>[episod, sue, sylvest, shuffl, air, ?]</td>\n",
       "      <td>[music, perform, also, attract, mix, commentar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>When was Cadillac founded?</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>english</td>\n",
       "      <td>Cadillac is among the first automobile brands ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cadillac</td>\n",
       "      <td>188</td>\n",
       "      <td>1902</td>\n",
       "      <td>True</td>\n",
       "      <td>[When, was, Cadillac, founded, ?]</td>\n",
       "      <td>[Cadillac, is, among, the, first, automobile, ...</td>\n",
       "      <td>[cadillac, found, ?]</td>\n",
       "      <td>[cadillac, among, first, automobil, brand, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>How many cricket teams are in Australia?</td>\n",
       "      <td>Cricket in Australia</td>\n",
       "      <td>english</td>\n",
       "      <td>The 2015 Cricket World Cup was jointly hosted ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cricket%20in%20A...</td>\n",
       "      <td>110</td>\n",
       "      <td>Fourteen</td>\n",
       "      <td>True</td>\n",
       "      <td>[How, many, cricket, teams, are, in, Australia...</td>\n",
       "      <td>[The, 2015, Cricket, World, Cup, was, jointly,...</td>\n",
       "      <td>[mani, cricket, team, australia, ?]</td>\n",
       "      <td>[2015, cricket, world, cup, joint, host, austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7283</th>\n",
       "      <td>Who owns the AMC network?</td>\n",
       "      <td>AMC Networks</td>\n",
       "      <td>english</td>\n",
       "      <td>Rainbow ran the local-minded MSG Metro Channel...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/AMC%20Networks</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[Who, owns, the, AMC, network, ?]</td>\n",
       "      <td>[Rainbow, ran, the, local-minded, MSG, Metro, ...</td>\n",
       "      <td>[own, amc, network, ?]</td>\n",
       "      <td>[rainbow, ran, local-mind, msg, metro, channel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>What was the first hormone discovered?</td>\n",
       "      <td>Secretin</td>\n",
       "      <td>english</td>\n",
       "      <td>It has been suggested that abnormalities in su...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Secretin</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, was, the, first, hormone, discovered, ?]</td>\n",
       "      <td>[It, has, been, suggested, that, abnormalities...</td>\n",
       "      <td>[first, hormon, discov, ?]</td>\n",
       "      <td>[suggest, abnorm, secretin, releas, could, exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>What was the first country to adopt socialist ...</td>\n",
       "      <td>History of socialism</td>\n",
       "      <td>english</td>\n",
       "      <td>The initial success of the Russian Revolution ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/History%20of%20s...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, was, the, first, country, to, adopt, so...</td>\n",
       "      <td>[The, initial, success, of, the, Russian, Revo...</td>\n",
       "      <td>[first, countri, adopt, socialist, principl, ?]</td>\n",
       "      <td>[initi, success, russian, revolut, inspir, rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>What station did Monk air on?</td>\n",
       "      <td>Monk (TV series)</td>\n",
       "      <td>english</td>\n",
       "      <td>A \"behind the scenes\" audio podcast entitled \"...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Monk%20%28TV%20s...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, station, did, Monk, air, on, ?]</td>\n",
       "      <td>[A, ``, behind, the, scenes, '', audio, podcas...</td>\n",
       "      <td>[station, monk, air, ?]</td>\n",
       "      <td>[``, behind, scene, '', audio, podcast, entitl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>What is the most common element in the Earth's...</td>\n",
       "      <td>Abundance of the chemical elements</td>\n",
       "      <td>english</td>\n",
       "      <td>The elements â€“ that is, ordinary (baryonic) ma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abundance%20of%2...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, is, the, most, common, element, in, the...</td>\n",
       "      <td>[The, elements, â€“, that, is, ,, ordinary, (, b...</td>\n",
       "      <td>[common, element, earth, 's, atmospher, ?]</td>\n",
       "      <td>[element, â€“, ,, ordinari, (, baryon, ), matter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>Who won the last Super Bowl?</td>\n",
       "      <td>List of Super Bowl champions</td>\n",
       "      <td>english</td>\n",
       "      <td>The Pittsburgh Steelers (6â€“2) have won the mos...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Supe...</td>\n",
       "      <td>94</td>\n",
       "      <td>New England Patriots</td>\n",
       "      <td>True</td>\n",
       "      <td>[Who, won, the, last, Super, Bowl, ?]</td>\n",
       "      <td>[The, Pittsburgh, Steelers, (, 6â€“2, ), have, w...</td>\n",
       "      <td>[last, super, bowl, ?]</td>\n",
       "      <td>[pittsburgh, steeler, (, 6â€“2, ), super, bowl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>How many pieces are used in backgammon?</td>\n",
       "      <td>Backgammon</td>\n",
       "      <td>english</td>\n",
       "      <td>Backgammon is one of the oldest known board ga...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Backgammon</td>\n",
       "      <td>222</td>\n",
       "      <td>fifteen</td>\n",
       "      <td>True</td>\n",
       "      <td>[How, many, pieces, are, used, in, backgammon, ?]</td>\n",
       "      <td>[Backgammon, is, one, of, the, oldest, known, ...</td>\n",
       "      <td>[mani, piec, use, backgammon, ?]</td>\n",
       "      <td>[backgammon, one, oldest, known, board, game, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6700 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question_text  \\\n",
       "5415  When did the episode The Sue Sylvester Shuffle...   \n",
       "751                          When was Cadillac founded?   \n",
       "3774           How many cricket teams are in Australia?   \n",
       "7283                          Who owns the AMC network?   \n",
       "4382             What was the first hormone discovered?   \n",
       "...                                                 ...   \n",
       "4373  What was the first country to adopt socialist ...   \n",
       "7891                      What station did Monk air on?   \n",
       "4859  What is the most common element in the Earth's...   \n",
       "3264                       Who won the last Super Bowl?   \n",
       "2732            How many pieces are used in backgammon?   \n",
       "\n",
       "                          document_title language  \\\n",
       "5415           The Sue Sylvester Shuffle  english   \n",
       "751                             Cadillac  english   \n",
       "3774                Cricket in Australia  english   \n",
       "7283                        AMC Networks  english   \n",
       "4382                            Secretin  english   \n",
       "...                                  ...      ...   \n",
       "4373                History of socialism  english   \n",
       "7891                    Monk (TV series)  english   \n",
       "4859  Abundance of the chemical elements  english   \n",
       "3264        List of Super Bowl champions  english   \n",
       "2732                          Backgammon  english   \n",
       "\n",
       "                                     document_plaintext  \\\n",
       "5415  Musical performances also attracted mixed comm...   \n",
       "751   Cadillac is among the first automobile brands ...   \n",
       "3774  The 2015 Cricket World Cup was jointly hosted ...   \n",
       "7283  Rainbow ran the local-minded MSG Metro Channel...   \n",
       "4382  It has been suggested that abnormalities in su...   \n",
       "...                                                 ...   \n",
       "4373  The initial success of the Russian Revolution ...   \n",
       "7891  A \"behind the scenes\" audio podcast entitled \"...   \n",
       "4859  The elements â€“ that is, ordinary (baryonic) ma...   \n",
       "3264  The Pittsburgh Steelers (6â€“2) have won the mos...   \n",
       "2732  Backgammon is one of the oldest known board ga...   \n",
       "\n",
       "                                           document_url  answer_start  \\\n",
       "5415  https://en.wikipedia.org/wiki/The%20Sue%20Sylv...            -1   \n",
       "751              https://en.wikipedia.org/wiki/Cadillac           188   \n",
       "3774  https://en.wikipedia.org/wiki/Cricket%20in%20A...           110   \n",
       "7283       https://en.wikipedia.org/wiki/AMC%20Networks            -1   \n",
       "4382             https://en.wikipedia.org/wiki/Secretin            -1   \n",
       "...                                                 ...           ...   \n",
       "4373  https://en.wikipedia.org/wiki/History%20of%20s...            -1   \n",
       "7891  https://en.wikipedia.org/wiki/Monk%20%28TV%20s...            -1   \n",
       "4859  https://en.wikipedia.org/wiki/Abundance%20of%2...            -1   \n",
       "3264  https://en.wikipedia.org/wiki/List%20of%20Supe...            94   \n",
       "2732           https://en.wikipedia.org/wiki/Backgammon           222   \n",
       "\n",
       "               answer_text  is_answerable  \\\n",
       "5415                                False   \n",
       "751                   1902           True   \n",
       "3774              Fourteen           True   \n",
       "7283                                False   \n",
       "4382                                False   \n",
       "...                    ...            ...   \n",
       "4373                                False   \n",
       "7891                                False   \n",
       "4859                                False   \n",
       "3264  New England Patriots           True   \n",
       "2732               fifteen           True   \n",
       "\n",
       "                                     tokenized_question  \\\n",
       "5415  [When, did, the, episode, The, Sue, Sylvester,...   \n",
       "751                   [When, was, Cadillac, founded, ?]   \n",
       "3774  [How, many, cricket, teams, are, in, Australia...   \n",
       "7283                  [Who, owns, the, AMC, network, ?]   \n",
       "4382    [What, was, the, first, hormone, discovered, ?]   \n",
       "...                                                 ...   \n",
       "4373  [What, was, the, first, country, to, adopt, so...   \n",
       "7891             [What, station, did, Monk, air, on, ?]   \n",
       "4859  [What, is, the, most, common, element, in, the...   \n",
       "3264              [Who, won, the, last, Super, Bowl, ?]   \n",
       "2732  [How, many, pieces, are, used, in, backgammon, ?]   \n",
       "\n",
       "                                    tokenized_plaintext  \\\n",
       "5415  [Musical, performances, also, attracted, mixed...   \n",
       "751   [Cadillac, is, among, the, first, automobile, ...   \n",
       "3774  [The, 2015, Cricket, World, Cup, was, jointly,...   \n",
       "7283  [Rainbow, ran, the, local-minded, MSG, Metro, ...   \n",
       "4382  [It, has, been, suggested, that, abnormalities...   \n",
       "...                                                 ...   \n",
       "4373  [The, initial, success, of, the, Russian, Revo...   \n",
       "7891  [A, ``, behind, the, scenes, '', audio, podcas...   \n",
       "4859  [The, elements, â€“, that, is, ,, ordinary, (, b...   \n",
       "3264  [The, Pittsburgh, Steelers, (, 6â€“2, ), have, w...   \n",
       "2732  [Backgammon, is, one, of, the, oldest, known, ...   \n",
       "\n",
       "                                     cleaned_question  \\\n",
       "5415           [episod, sue, sylvest, shuffl, air, ?]   \n",
       "751                              [cadillac, found, ?]   \n",
       "3774              [mani, cricket, team, australia, ?]   \n",
       "7283                           [own, amc, network, ?]   \n",
       "4382                       [first, hormon, discov, ?]   \n",
       "...                                               ...   \n",
       "4373  [first, countri, adopt, socialist, principl, ?]   \n",
       "7891                          [station, monk, air, ?]   \n",
       "4859       [common, element, earth, 's, atmospher, ?]   \n",
       "3264                           [last, super, bowl, ?]   \n",
       "2732                 [mani, piec, use, backgammon, ?]   \n",
       "\n",
       "                                      cleaned_plaintext  \n",
       "5415  [music, perform, also, attract, mix, commentar...  \n",
       "751   [cadillac, among, first, automobil, brand, wor...  \n",
       "3774  [2015, cricket, world, cup, joint, host, austr...  \n",
       "7283  [rainbow, ran, local-mind, msg, metro, channel...  \n",
       "4382  [suggest, abnorm, secretin, releas, could, exp...  \n",
       "...                                                 ...  \n",
       "4373  [initi, success, russian, revolut, inspir, rev...  \n",
       "7891  [``, behind, scene, '', audio, podcast, entitl...  \n",
       "4859  [element, â€“, ,, ordinari, (, baryon, ), matter...  \n",
       "3264  [pittsburgh, steeler, (, 6â€“2, ), super, bowl, ...  \n",
       "2732  [backgammon, one, oldest, known, board, game, ...  \n",
       "\n",
       "[6700 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"english\"][\"train_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from transformers import BertTokenizer, BertLMHeadModel\n",
    "# import torch\n",
    "# from torch.nn import functional as F\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "# model = BertLMHeadModel.from_pretrained('bert-base-multilingual-uncased',\n",
    "# return_dict=True, is_decoder = True)\n",
    "# text = \"A knife is very \"\n",
    "# input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "# output = model(**input).logits[:, -1, :]\n",
    "# softmax = F.softmax(output, -1)\n",
    "# index = torch.argmax(softmax, dim = -1)\n",
    "# x = tokenizer.decode(index)\n",
    "# print(\"word:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertLMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertLMHeadModel.from_pretrained('bert-base-multilingual-uncased', is_decoder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = all_data[\"english\"][\"train_data\"]\n",
    "dataset = dataset.head(10)\n",
    "train_dataset = Dataset.from_pandas(dataset[['question_text', 'document_plaintext']])\n",
    "val_dataset =  Dataset.from_pandas(dataset[['question_text', 'document_plaintext']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8576095766e145848b31cf0e757a0a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70d2a1b10294207bf20e0af374402d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_str = 'Question: ' + \\\n",
    "                examples['question_text'] + '\\nContext: ' + \\\n",
    "                examples['document_plaintext']\n",
    "    return tokenizer(input_str, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=['question_text', 'document_plaintext'],\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=['question_text', 'document_plaintext'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_headlines_path = './model_headlines_news'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_headlines_path,          # output directory\n",
    "    num_train_epochs=6,              # total # of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=model_headlines_path,            # directory for storing logs\n",
    "    prediction_loss_only=True,\n",
    "    save_steps=10000 \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertLMHeadModel.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertLMHeadModel.forward`,  you can safely ignore this message.\n",
      "/Users/axelhojmark/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef456aad4938401b879de95931941c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,         # training dataset\n",
    "    eval_dataset=tokenized_val_dataset            # evaluation dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   143, 65016, 10127, 12495,   102,   143,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119],\n",
       "        [  101,   143, 65016, 10127, 12495,   102,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119],\n",
       "        [  101,   143, 65016, 10127, 12495,   102,   157,   157,   157,   157,\n",
       "           157,   157,   157,   157,   157,   157,   157,   157,   157,   157,\n",
       "           157,   157,   157,   157,   157,   157,   157,   157,   157,   157,\n",
       "           157,   157,   157,   157,   157,   157,   157,   157,   157,   157,\n",
       "           157,   157,   157,   157,   157,   157,   157,   157,   157,   157],\n",
       "        [  101,   143, 65016, 10127, 12495,   102, 10102, 10102, 10102, 10102,\n",
       "         10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102,\n",
       "         10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102,\n",
       "         10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102,\n",
       "         10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102],\n",
       "        [  101,   143, 65016, 10127, 12495,   102,   117,   117,   117,   117,\n",
       "           117,   117,   117,   117,   117,   117,   117,   117,   117,   117,\n",
       "           117,   117,   117,   117,   117,   117,   117,   117,   117,   117,\n",
       "           117,   117,   117,   117,   117,   117,   117,   117,   117,   117,\n",
       "           117,   117,   117,   117,   117,   117,   117,   117,   117,   117]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"A knife is very \"\n",
    "text_ids = tokenizer.encode(text, return_tensors = 'pt')\n",
    "\n",
    "generated_text_samples = model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    num_beams=5,\n",
    "    num_return_sequences= 5,\n",
    "    early_stopping=True \n",
    ")\n",
    "generated_text_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: a knife is very a...........................................\n",
      "\n",
      "1: a knife is very............................................\n",
      "\n",
      "2: a knife is very o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o\n",
      "\n",
      "3: a knife is very de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "4: a knife is very,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, beam in enumerate(generated_text_samples):\n",
    "    print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing our binary question system with a multilingual encoder instead of the monolingual ones. With this we perform zero-shot cross-lingual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Training on english ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdc41aed3054e3ba408bcf75226355e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035f34019ed94931971cbc891e933de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b044592ded4bf7aafa07f0f25ce985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5aaeb1899948248ff579fc525d3b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/641M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8s/v3zv16l965jfcv5vbcctc1qw0000gn/T/ipykernel_5494/2249998659.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NLP-2022/models/Logistic/Logistic.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaintext_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_word_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_save_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/MultiGPT2Logistic/english.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8s/v3zv16l965jfcv5vbcctc1qw0000gn/T/ipykernel_5494/2249998659.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_language\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_answerable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         model = pipeline.train(\n",
      "\u001b[0;32m~/Desktop/NLP-2022/models/feature_extraction/MultiGPT2Feature.py\u001b[0m in \u001b[0;36mextract_X\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPT2Generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_GPT2Generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         gpt2_output = self.GPT2Generator.predict(\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPT2Generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NLP-2022/models/feature_extraction/MultiGPT2Feature.py\u001b[0m in \u001b[0;36mget_GPT2Generator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_GPT2Generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mgpt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiGPT2Generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NLP-2022/models/MultiGPT2Generator.py\u001b[0m in \u001b[0;36mset_language\u001b[0;34m(self, language)\u001b[0m\n\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         self.model = BertLMHeadModel.from_pretrained(\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mpretrained_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m                 \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[1;32m   1941\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m                     \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# The url_to_download might be messy, so we extract the file name from the original url.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, file_name)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Downloading {file_name}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"Downloading\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     )\n\u001b[0;32m--> 453\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m                 if (\n\u001b[1;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Zero shot classification\n",
    "for train_language in languages:\n",
    "    print(f'\\n\\n--- Training on {train_language.name} ---')\n",
    "    model = MultiGPT2Logistic()\n",
    "    model.set_language(train_language.name)\n",
    "    \n",
    "    try:\n",
    "        model.load()\n",
    "    except:\n",
    "        train_data = all_data[train_language.name][\"train_data\"]\n",
    "        X_train = model.extract_X(train_data)\n",
    "        y_train = train_data['is_answerable']\n",
    "        model = pipeline.train(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train\n",
    "        )\n",
    "        model.save()\n",
    "    \n",
    "    for val_language in languages:\n",
    "        print(f'\\n\\t- Validating on {val_language.name}')\n",
    "        validation_data = all_data[val_language.name][\"validation_data\"]\n",
    "        X_validation = model.extract_X(validation_data)\n",
    "        y_validation = validation_data['is_answerable']\n",
    "        pipeline.evaluate(\n",
    "            model,\n",
    "            X_validation,\n",
    "            y_validation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9c69bc24335dc0e55392644c8f34f14710bb68159a638262db0555ff8c8aa69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
