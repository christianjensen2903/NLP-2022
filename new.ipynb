{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pipeline import Pipeline\n",
    "from Language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datasets import load_dataset\n",
    "# import fugashi\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer                           \n",
    "from nltk.tokenize import word_tokenize\n",
    "import libvoikko\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christianjensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-2c79d6e77df16c2a\n",
      "Reusing dataset parquet (/Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 228.62it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('copenlu/answerable_tydiqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-966bf95a10f76383.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e0b012db28a0a682.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-57253105caae3cc1.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-efce6354aa2f51ed.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b5a509467a2a492a.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ead531b3ac5c84fa.arrow\n"
     ]
    }
   ],
   "source": [
    "get_data = lambda language: dataset.filter(lambda x: x['language'] == language)\n",
    "\n",
    "# define languages\n",
    "languages = {\n",
    "    'english': Language(\n",
    "        name = 'english',\n",
    "        tokenizer = word_tokenize,\n",
    "        cleaner = lambda text: text.lower(),\n",
    "        pipeline = Pipeline(get_data('english'))\n",
    "        ),\n",
    "    'japanese': Language(\n",
    "        name = 'japanese',\n",
    "        tokenizer = lambda text: text.split(), #[word.surface for word in fugashi.Tagger()(text)],\n",
    "        cleaner = lambda text: text.lower(),\n",
    "        pipeline = Pipeline(get_data('japanese'))\n",
    "        ),\n",
    "    'finnish': Language(\n",
    "        name = 'finnish',\n",
    "        tokenizer = word_tokenize,\n",
    "        cleaner = lambda text: text.lower(),\n",
    "        pipeline = Pipeline(get_data('finnish'))\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-dbd65a657c089916.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4e47629646f1b6b1.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-95adf22fa0d997be.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-04aed0f9be862f0e.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-507001bf61297f1f.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9ecd618a7bbab136.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-eedb3041cf97685f.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-0974ee507581e3c2.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3d64c4f70e89659a.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-fe502946f86f4815.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8d147f3d68a6dded.arrow\n",
      "Loading cached processed dataset at /Users/christianjensen/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-2c79d6e77df16c2a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b0cd97409abf53d1.arrow\n"
     ]
    }
   ],
   "source": [
    "for language in languages.values():\n",
    "    language.pipeline.tokenize(language.tokenizer)\n",
    "    language.pipeline.label_answerable()\n",
    "    language.pipeline.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Language: english\n",
      "    Most frequent first words:\n",
      "    [('When', 2242), ('What', 2101), ('How', 1296), ('Who', 1058), ('Where', 486)]\n",
      "    Most frequent last words:\n",
      "    [('?', 7379), ('zombie', 2), ('metabolite', 2), ('\\\\', 2), ('BCE', 2)]\n",
      "    \n",
      "\n",
      "    Language: japanese\n",
      "    Most frequent first words:\n",
      "    [('朝比奈', 6), ('三原', 6), ('孫', 6), ('PlayStation', 6), ('加藤', 6)]\n",
      "    Most frequent last words:\n",
      "    [('の大きさは？', 4), ('の面積は？', 4), ('はいつ設立した？', 4), ('ＹＡＭＡＨＡがピアノの生産を始めたのはいつ', 2), ('ソ連が崩壊したのはいつ', 2)]\n",
      "    \n",
      "\n",
      "    Language: finnish\n",
      "    Most frequent first words:\n",
      "    [('Milloin', 3519), ('Mikä', 2328), ('Missä', 1646), ('Kuka', 1619), ('Mitä', 1088)]\n",
      "    Most frequent last words:\n",
      "    [('?', 13689), ('tulitaistelussa', 2), ('tohtoriksi+', 2), ('syntynyt', 2), ('pinta-ala', 2)]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Find the most common first and last words in each language\n",
    "for language in languages.values():\n",
    "    count_words = lambda text: np.unique(text, return_counts=True) # Count occurences of words in text\n",
    "    sort_words = lambda word_count: np.argsort(word_count[1])[::-1] # Get list of sorted indices based on most frequent words\n",
    "    zip_words = lambda word_counts, sort_indices: list(zip(word_counts[0][sort_indices],word_counts[1][sort_indices])) # Zip the most frequent words with its number of occurences\n",
    "    def find_most_common(text):\n",
    "        \"\"\"Finds the most frequent words in a text together with its number of occurences\"\"\"\n",
    "        word_count = count_words(text)\n",
    "        return zip_words(word_count, sort_words(word_count))\n",
    "\n",
    "\n",
    "    tokenized_questions = language.pipeline.train_data['tokenized_question']\n",
    "    first = [sublist[0] for sublist in tokenized_questions]\n",
    "    last = [sublist[-1] for sublist in tokenized_questions]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Language: {language.name}\n",
    "    Most frequent first words:\n",
    "    {find_most_common(first)[:5]}\n",
    "    Most frequent last words:\n",
    "    {find_most_common(last)[:5]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "test2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_13662/664012335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2 - Projects/NLP Course/NLP-2022/Pipeline.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training accuracy: {self.model.score(self.X, self.Y)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1555\u001b[0m                 \u001b[0;34m\"This solver needs samples of at least 2 classes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "for language in languages.values():\n",
    "    language.pipeline.train(LogisticRegression())\n",
    "    language.pipeline.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
