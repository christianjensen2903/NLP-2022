
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
tensor([[[-0.0708,  0.0921,  0.2836],
         [-0.5768, -0.3638,  0.2972],
         [-0.2174, -0.0517,  0.0877],
         ...,
         [-0.2690,  0.0718,  0.0601],
         [-0.2709,  0.0544,  0.0245],
         [-0.1550,  0.0544,  0.0376]],
        [[-0.2691,  0.1720,  0.1536],
         [ 0.0342, -0.6769,  0.0204],
         [ 0.1661,  0.0648, -0.0204],
         ...,
         [-0.2106,  0.0349,  0.0199],
         [-0.2460,  0.2500,  0.1275],
         [ 0.0395,  0.0508,  0.0321]],
        [[-0.0399,  0.1059,  0.5372],
         [-0.1324, -0.2875,  0.3406],
         [ 0.1860, -0.0574,  0.2730],
         ...,
         [-0.1369,  0.2364,  0.1946],
         [-0.0931,  0.0973,  0.2394],
         [-0.0229,  0.1699,  0.3280]],
        ...,
        [[-0.4945,  0.0343,  0.4119],
         [-0.5215, -0.5269,  0.0398],
         [ 0.4943, -0.1509,  0.0157],
         ...,
         [-0.0235,  0.0158, -0.0426],
         [-0.0253,  0.1213, -0.0040],
         [ 0.1222, -0.2910, -0.1189]],
        [[-0.4437, -0.3283,  0.1741],
         [-0.3044, -0.3457, -0.0129],
         [-0.0887,  0.0903,  0.1393],
         ...,
         [ 0.0153,  0.1152,  0.2834],
         [-0.0224,  0.0016,  0.2265],
         [ 0.0129,  0.0399,  0.3517]],
        [[-0.2057, -0.1847,  0.1902],
         [-0.0719, -0.2029,  0.3694],
         [ 0.0128,  0.0431, -0.0860],
         ...,
         [ 0.0827,  0.0596, -0.0175],
         [ 0.0384,  0.1452,  0.0719],
         [-0.0503, -0.1514, -0.1070]]], grad_fn=<AddBackward0>)