{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 17:44:02.856811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from models.Model import Model\n",
    "from models.Logistic.MultiGPT2Logistic import MultiGPT2Logistic\n",
    "from languages.LanguageModel import LanguageModel\n",
    "from DataExploration import DataExploration\n",
    "# from languages.Japanese import Japanese\n",
    "from languages.English import English\n",
    "from languages.Finnish import Finnish\n",
    "from Preprocess import Preprocess\n",
    "from Pipeline import Pipeline\n",
    "from typing import List\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently \n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is used to minimize the clutter in the console\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "# Define the languages to be used\n",
    "languages: List[LanguageModel] = [\n",
    "    English(),\n",
    "    Finnish(),\n",
    "    # Japanese()\n",
    "]\n",
    "\n",
    "# gpt2Generator = GPT2Generator()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "all_data = {}\n",
    "\n",
    "for language in languages:\n",
    "    pipeline = Pipeline()\n",
    "\n",
    "    # Get the preprocessed data and split it into training and validation data\n",
    "    preprocessor = Preprocess(language.tokenize, language.clean)\n",
    "    data = pipeline.get_data(language=language.name, preproccesor=preprocessor)\n",
    "    train_data, validation_data = pipeline.split_data(data)\n",
    "\n",
    "    all_data[language.name] = {\n",
    "        \"train_data\": train_data,\n",
    "        \"validation_data\": validation_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>is_answerable</th>\n",
       "      <th>tokenized_question</th>\n",
       "      <th>tokenized_plaintext</th>\n",
       "      <th>cleaned_question</th>\n",
       "      <th>cleaned_plaintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>When did the episode The Sue Sylvester Shuffle...</td>\n",
       "      <td>The Sue Sylvester Shuffle</td>\n",
       "      <td>english</td>\n",
       "      <td>Musical performances also attracted mixed comm...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The%20Sue%20Sylv...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[When, did, the, episode, The, Sue, Sylvester,...</td>\n",
       "      <td>[Musical, performances, also, attracted, mixed...</td>\n",
       "      <td>[episod, sue, sylvest, shuffl, air, ?]</td>\n",
       "      <td>[music, perform, also, attract, mix, commentar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>When was Cadillac founded?</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>english</td>\n",
       "      <td>Cadillac is among the first automobile brands ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cadillac</td>\n",
       "      <td>188</td>\n",
       "      <td>1902</td>\n",
       "      <td>True</td>\n",
       "      <td>[When, was, Cadillac, founded, ?]</td>\n",
       "      <td>[Cadillac, is, among, the, first, automobile, ...</td>\n",
       "      <td>[cadillac, found, ?]</td>\n",
       "      <td>[cadillac, among, first, automobil, brand, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>How many cricket teams are in Australia?</td>\n",
       "      <td>Cricket in Australia</td>\n",
       "      <td>english</td>\n",
       "      <td>The 2015 Cricket World Cup was jointly hosted ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cricket%20in%20A...</td>\n",
       "      <td>110</td>\n",
       "      <td>Fourteen</td>\n",
       "      <td>True</td>\n",
       "      <td>[How, many, cricket, teams, are, in, Australia...</td>\n",
       "      <td>[The, 2015, Cricket, World, Cup, was, jointly,...</td>\n",
       "      <td>[mani, cricket, team, australia, ?]</td>\n",
       "      <td>[2015, cricket, world, cup, joint, host, austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7283</th>\n",
       "      <td>Who owns the AMC network?</td>\n",
       "      <td>AMC Networks</td>\n",
       "      <td>english</td>\n",
       "      <td>Rainbow ran the local-minded MSG Metro Channel...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/AMC%20Networks</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[Who, owns, the, AMC, network, ?]</td>\n",
       "      <td>[Rainbow, ran, the, local-minded, MSG, Metro, ...</td>\n",
       "      <td>[own, amc, network, ?]</td>\n",
       "      <td>[rainbow, ran, local-mind, msg, metro, channel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>What was the first hormone discovered?</td>\n",
       "      <td>Secretin</td>\n",
       "      <td>english</td>\n",
       "      <td>It has been suggested that abnormalities in su...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Secretin</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, was, the, first, hormone, discovered, ?]</td>\n",
       "      <td>[It, has, been, suggested, that, abnormalities...</td>\n",
       "      <td>[first, hormon, discov, ?]</td>\n",
       "      <td>[suggest, abnorm, secretin, releas, could, exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>What was the first country to adopt socialist ...</td>\n",
       "      <td>History of socialism</td>\n",
       "      <td>english</td>\n",
       "      <td>The initial success of the Russian Revolution ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/History%20of%20s...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, was, the, first, country, to, adopt, so...</td>\n",
       "      <td>[The, initial, success, of, the, Russian, Revo...</td>\n",
       "      <td>[first, countri, adopt, socialist, principl, ?]</td>\n",
       "      <td>[initi, success, russian, revolut, inspir, rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>What station did Monk air on?</td>\n",
       "      <td>Monk (TV series)</td>\n",
       "      <td>english</td>\n",
       "      <td>A \"behind the scenes\" audio podcast entitled \"...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Monk%20%28TV%20s...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, station, did, Monk, air, on, ?]</td>\n",
       "      <td>[A, ``, behind, the, scenes, '', audio, podcas...</td>\n",
       "      <td>[station, monk, air, ?]</td>\n",
       "      <td>[``, behind, scene, '', audio, podcast, entitl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>What is the most common element in the Earth's...</td>\n",
       "      <td>Abundance of the chemical elements</td>\n",
       "      <td>english</td>\n",
       "      <td>The elements – that is, ordinary (baryonic) ma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abundance%20of%2...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[What, is, the, most, common, element, in, the...</td>\n",
       "      <td>[The, elements, –, that, is, ,, ordinary, (, b...</td>\n",
       "      <td>[common, element, earth, 's, atmospher, ?]</td>\n",
       "      <td>[element, –, ,, ordinari, (, baryon, ), matter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>Who won the last Super Bowl?</td>\n",
       "      <td>List of Super Bowl champions</td>\n",
       "      <td>english</td>\n",
       "      <td>The Pittsburgh Steelers (6–2) have won the mos...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Supe...</td>\n",
       "      <td>94</td>\n",
       "      <td>New England Patriots</td>\n",
       "      <td>True</td>\n",
       "      <td>[Who, won, the, last, Super, Bowl, ?]</td>\n",
       "      <td>[The, Pittsburgh, Steelers, (, 6–2, ), have, w...</td>\n",
       "      <td>[last, super, bowl, ?]</td>\n",
       "      <td>[pittsburgh, steeler, (, 6–2, ), super, bowl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>How many pieces are used in backgammon?</td>\n",
       "      <td>Backgammon</td>\n",
       "      <td>english</td>\n",
       "      <td>Backgammon is one of the oldest known board ga...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Backgammon</td>\n",
       "      <td>222</td>\n",
       "      <td>fifteen</td>\n",
       "      <td>True</td>\n",
       "      <td>[How, many, pieces, are, used, in, backgammon, ?]</td>\n",
       "      <td>[Backgammon, is, one, of, the, oldest, known, ...</td>\n",
       "      <td>[mani, piec, use, backgammon, ?]</td>\n",
       "      <td>[backgammon, one, oldest, known, board, game, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6700 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question_text  \\\n",
       "5415  When did the episode The Sue Sylvester Shuffle...   \n",
       "751                          When was Cadillac founded?   \n",
       "3774           How many cricket teams are in Australia?   \n",
       "7283                          Who owns the AMC network?   \n",
       "4382             What was the first hormone discovered?   \n",
       "...                                                 ...   \n",
       "4373  What was the first country to adopt socialist ...   \n",
       "7891                      What station did Monk air on?   \n",
       "4859  What is the most common element in the Earth's...   \n",
       "3264                       Who won the last Super Bowl?   \n",
       "2732            How many pieces are used in backgammon?   \n",
       "\n",
       "                          document_title language  \\\n",
       "5415           The Sue Sylvester Shuffle  english   \n",
       "751                             Cadillac  english   \n",
       "3774                Cricket in Australia  english   \n",
       "7283                        AMC Networks  english   \n",
       "4382                            Secretin  english   \n",
       "...                                  ...      ...   \n",
       "4373                History of socialism  english   \n",
       "7891                    Monk (TV series)  english   \n",
       "4859  Abundance of the chemical elements  english   \n",
       "3264        List of Super Bowl champions  english   \n",
       "2732                          Backgammon  english   \n",
       "\n",
       "                                     document_plaintext  \\\n",
       "5415  Musical performances also attracted mixed comm...   \n",
       "751   Cadillac is among the first automobile brands ...   \n",
       "3774  The 2015 Cricket World Cup was jointly hosted ...   \n",
       "7283  Rainbow ran the local-minded MSG Metro Channel...   \n",
       "4382  It has been suggested that abnormalities in su...   \n",
       "...                                                 ...   \n",
       "4373  The initial success of the Russian Revolution ...   \n",
       "7891  A \"behind the scenes\" audio podcast entitled \"...   \n",
       "4859  The elements – that is, ordinary (baryonic) ma...   \n",
       "3264  The Pittsburgh Steelers (6–2) have won the mos...   \n",
       "2732  Backgammon is one of the oldest known board ga...   \n",
       "\n",
       "                                           document_url  answer_start  \\\n",
       "5415  https://en.wikipedia.org/wiki/The%20Sue%20Sylv...            -1   \n",
       "751              https://en.wikipedia.org/wiki/Cadillac           188   \n",
       "3774  https://en.wikipedia.org/wiki/Cricket%20in%20A...           110   \n",
       "7283       https://en.wikipedia.org/wiki/AMC%20Networks            -1   \n",
       "4382             https://en.wikipedia.org/wiki/Secretin            -1   \n",
       "...                                                 ...           ...   \n",
       "4373  https://en.wikipedia.org/wiki/History%20of%20s...            -1   \n",
       "7891  https://en.wikipedia.org/wiki/Monk%20%28TV%20s...            -1   \n",
       "4859  https://en.wikipedia.org/wiki/Abundance%20of%2...            -1   \n",
       "3264  https://en.wikipedia.org/wiki/List%20of%20Supe...            94   \n",
       "2732           https://en.wikipedia.org/wiki/Backgammon           222   \n",
       "\n",
       "               answer_text  is_answerable  \\\n",
       "5415                                False   \n",
       "751                   1902           True   \n",
       "3774              Fourteen           True   \n",
       "7283                                False   \n",
       "4382                                False   \n",
       "...                    ...            ...   \n",
       "4373                                False   \n",
       "7891                                False   \n",
       "4859                                False   \n",
       "3264  New England Patriots           True   \n",
       "2732               fifteen           True   \n",
       "\n",
       "                                     tokenized_question  \\\n",
       "5415  [When, did, the, episode, The, Sue, Sylvester,...   \n",
       "751                   [When, was, Cadillac, founded, ?]   \n",
       "3774  [How, many, cricket, teams, are, in, Australia...   \n",
       "7283                  [Who, owns, the, AMC, network, ?]   \n",
       "4382    [What, was, the, first, hormone, discovered, ?]   \n",
       "...                                                 ...   \n",
       "4373  [What, was, the, first, country, to, adopt, so...   \n",
       "7891             [What, station, did, Monk, air, on, ?]   \n",
       "4859  [What, is, the, most, common, element, in, the...   \n",
       "3264              [Who, won, the, last, Super, Bowl, ?]   \n",
       "2732  [How, many, pieces, are, used, in, backgammon, ?]   \n",
       "\n",
       "                                    tokenized_plaintext  \\\n",
       "5415  [Musical, performances, also, attracted, mixed...   \n",
       "751   [Cadillac, is, among, the, first, automobile, ...   \n",
       "3774  [The, 2015, Cricket, World, Cup, was, jointly,...   \n",
       "7283  [Rainbow, ran, the, local-minded, MSG, Metro, ...   \n",
       "4382  [It, has, been, suggested, that, abnormalities...   \n",
       "...                                                 ...   \n",
       "4373  [The, initial, success, of, the, Russian, Revo...   \n",
       "7891  [A, ``, behind, the, scenes, '', audio, podcas...   \n",
       "4859  [The, elements, –, that, is, ,, ordinary, (, b...   \n",
       "3264  [The, Pittsburgh, Steelers, (, 6–2, ), have, w...   \n",
       "2732  [Backgammon, is, one, of, the, oldest, known, ...   \n",
       "\n",
       "                                     cleaned_question  \\\n",
       "5415           [episod, sue, sylvest, shuffl, air, ?]   \n",
       "751                              [cadillac, found, ?]   \n",
       "3774              [mani, cricket, team, australia, ?]   \n",
       "7283                           [own, amc, network, ?]   \n",
       "4382                       [first, hormon, discov, ?]   \n",
       "...                                               ...   \n",
       "4373  [first, countri, adopt, socialist, principl, ?]   \n",
       "7891                          [station, monk, air, ?]   \n",
       "4859       [common, element, earth, 's, atmospher, ?]   \n",
       "3264                           [last, super, bowl, ?]   \n",
       "2732                 [mani, piec, use, backgammon, ?]   \n",
       "\n",
       "                                      cleaned_plaintext  \n",
       "5415  [music, perform, also, attract, mix, commentar...  \n",
       "751   [cadillac, among, first, automobil, brand, wor...  \n",
       "3774  [2015, cricket, world, cup, joint, host, austr...  \n",
       "7283  [rainbow, ran, local-mind, msg, metro, channel...  \n",
       "4382  [suggest, abnorm, secretin, releas, could, exp...  \n",
       "...                                                 ...  \n",
       "4373  [initi, success, russian, revolut, inspir, rev...  \n",
       "7891  [``, behind, scene, '', audio, podcast, entitl...  \n",
       "4859  [element, –, ,, ordinari, (, baryon, ), matter...  \n",
       "3264  [pittsburgh, steeler, (, 6–2, ), super, bowl, ...  \n",
       "2732  [backgammon, one, oldest, known, board, game, ...  \n",
       "\n",
       "[6700 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"english\"][\"train_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from transformers import BertTokenizer, BertLMHeadModel\n",
    "# import torch\n",
    "# from torch.nn import functional as F\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "# model = BertLMHeadModel.from_pretrained('bert-base-multilingual-uncased',\n",
    "# return_dict=True, is_decoder = True)\n",
    "# text = \"A knife is very \"\n",
    "# input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "# output = model(**input).logits[:, -1, :]\n",
    "# softmax = F.softmax(output, -1)\n",
    "# index = torch.argmax(softmax, dim = -1)\n",
    "# x = tokenizer.decode(index)\n",
    "# print(\"word:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertLMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertLMHeadModel.from_pretrained('bert-base-multilingual-uncased', is_decoder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = all_data[\"english\"][\"train_data\"]\n",
    "dataset = dataset.head(10)\n",
    "train_dataset = Dataset.from_pandas(dataset[['question_text', 'document_plaintext']])\n",
    "val_dataset =  Dataset.from_pandas(dataset[['question_text', 'document_plaintext']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8576095766e145848b31cf0e757a0a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70d2a1b10294207bf20e0af374402d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_str = 'Question: ' + \\\n",
    "                examples['question_text'] + '\\nContext: ' + \\\n",
    "                examples['document_plaintext']\n",
    "    return tokenizer(input_str, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=['question_text', 'document_plaintext'],\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=['question_text', 'document_plaintext'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_headlines_path = './model_headlines_news'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_headlines_path,          # output directory\n",
    "    num_train_epochs=6,              # total # of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=model_headlines_path,            # directory for storing logs\n",
    "    prediction_loss_only=True,\n",
    "    save_steps=10000 \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertLMHeadModel.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertLMHeadModel.forward`,  you can safely ignore this message.\n",
      "/Users/axelhojmark/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef456aad4938401b879de95931941c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,         # training dataset\n",
    "    eval_dataset=tokenized_val_dataset            # evaluation dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   143, 65016, 10127, 12495,   102,   143,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119],\n",
       "        [  101,   143, 65016, 10127, 12495,   102,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119],\n",
       "        [  101,   143, 65016, 10127, 12495,   102,   157,   157,   157,   157,\n",
       "           157,   157,   157,   157,   157,   157,   157,   157,   157,   157,\n",
       "           157,   157,   157,   157,   157,   157,   157,   157,   157,   157,\n",
       "           157,   157,   157,   157,   157,   157,   157,   157,   157,   157,\n",
       "           157,   157,   157,   157,   157,   157,   157,   157,   157,   157],\n",
       "        [  101,   143, 65016, 10127, 12495,   102, 10102, 10102, 10102, 10102,\n",
       "         10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102,\n",
       "         10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102,\n",
       "         10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102,\n",
       "         10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102, 10102],\n",
       "        [  101,   143, 65016, 10127, 12495,   102,   117,   117,   117,   117,\n",
       "           117,   117,   117,   117,   117,   117,   117,   117,   117,   117,\n",
       "           117,   117,   117,   117,   117,   117,   117,   117,   117,   117,\n",
       "           117,   117,   117,   117,   117,   117,   117,   117,   117,   117,\n",
       "           117,   117,   117,   117,   117,   117,   117,   117,   117,   117]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"A knife is very \"\n",
    "text_ids = tokenizer.encode(text, return_tensors = 'pt')\n",
    "\n",
    "generated_text_samples = model.generate(\n",
    "    text_ids,\n",
    "    max_length= 50,  \n",
    "    num_beams=5,\n",
    "    num_return_sequences= 5,\n",
    "    early_stopping=True \n",
    ")\n",
    "generated_text_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: a knife is very a...........................................\n",
      "\n",
      "1: a knife is very............................................\n",
      "\n",
      "2: a knife is very o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o\n",
      "\n",
      "3: a knife is very de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "4: a knife is very,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, beam in enumerate(generated_text_samples):\n",
    "    print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing our binary question system with a multilingual encoder instead of the monolingual ones. With this we perform zero-shot cross-lingual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Training on english ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d9fdd8fdb04c11a72ff0ea3af1c493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.81M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9684f5bf369a446599fd8946853f1825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/1.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c3d302cd354dc399d3c29ef3ca5301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a531d928b883490f89cbb49c70e720e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Zero shot classification\n",
    "for train_language in languages:\n",
    "    print(f'\\n\\n--- Training on {train_language.name} ---')\n",
    "    model = MultiGPT2Logistic()\n",
    "    model.set_language(train_language.name)\n",
    "    \n",
    "    try:\n",
    "        model.load()\n",
    "    except:\n",
    "        train_data = all_data[train_language.name][\"train_data\"]\n",
    "        X_train = model.extract_X(train_data)\n",
    "        y_train = train_data['is_answerable']\n",
    "        model = pipeline.train(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train\n",
    "        )\n",
    "        model.save()\n",
    "    \n",
    "    for val_language in languages:\n",
    "        print(f'\\n\\t- Validating on {val_language.name}')\n",
    "        validation_data = all_data[val_language.name][\"validation_data\"]\n",
    "        X_validation = model.extract_X(validation_data)\n",
    "        y_validation = validation_data['is_answerable']\n",
    "        pipeline.evaluate(\n",
    "            model,\n",
    "            X_validation,\n",
    "            y_validation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9c69bc24335dc0e55392644c8f34f14710bb68159a638262db0555ff8c8aa69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
